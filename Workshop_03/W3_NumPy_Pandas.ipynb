{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 03` - Libraries: NumPy & Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will introduce the concept of Python libraries and cover two very important such library - NumPy.\n",
    "\n",
    "We will go through the following:\n",
    "\n",
    "- Python dictionary and lists comprehensions\n",
    "- Introduction to the concept of `Python Libraries`\n",
    "- Introduction to `NumPy`\n",
    "- Introduction to `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Iterable Comprehensions\n",
    "\n",
    "Comprehensions are a method for transforming one iterable into another iterable. During this transformation, items within the original iterable can be conditionally included in the new one and each item can be transformed as needed. Comprehensions are a powerful concept and can be used to substitute `for loops` and `lambda functions`. However, not all `for loops` can be written as a comprehension but all comprehensions can be written with a `for loop`. (We will talk about `lambda functions` in future workshops).\n",
    "\n",
    "A good comprehension can make your code more expressive and, thus, easier to read. The key with creating comprehensions is to not let them get so complex that your head spins when you try to decipher what they are actually doing. Keeping the idea of *easy to read* alive.\n",
    "\n",
    "The general template you can follow for comprehensions in Python is:\n",
    "\n",
    "`dict_variable = {key:value for (key,value) in dictonary.items()}`\n",
    "\n",
    "`list_variable = [value for value in list]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'a':1, 'b':2, 'c':3, 'd':4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber that with the `items()` method you can access each key-value pair in a dictionary. You can access the values and keys in a dictionary by using `values()` and `keys()`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n",
      "dict_values([1, 2, 3, 4])\n",
      "dict_keys(['a', 'b', 'c', 'd'])\n"
     ]
    }
   ],
   "source": [
    "print(dict1.items())\n",
    "print(dict1.values())\n",
    "print(dict1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of dictionary comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 4, 'c': 9, 'd': 16}\n"
     ]
    }
   ],
   "source": [
    "# ^2 each value in the dictionary\n",
    "new_dict1 = {k:v**2 for (k,v) in dict1.items()}\n",
    "print(new_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd'}\n"
     ]
    }
   ],
   "source": [
    "# exchange keys and values\n",
    "new_dict2 = {v:k for (k,v) in dict1.items()}\n",
    "print(new_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa': 1, 'ba': 2, 'ca': 3, 'da': 4}\n"
     ]
    }
   ],
   "source": [
    "# change the keys of the original dictionary\n",
    "new_dict3 = {k+'a':v for (k,v) in dict1.items()}\n",
    "print(new_dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Comprehesions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following problem: you want to create a new dictionary with even numbers in a range of 0-10 as keys and the square of the number as values. One solution is to use a `for` loop. Another solution is to use dictionary comprehension method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 2: 4, 4: 16, 6: 36, 8: 64}\n"
     ]
    }
   ],
   "source": [
    "new_dict4 = {n:n**2 for n in range(10) if n%2 == 0}\n",
    "print(new_dict4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same with any other iterable, especially lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls1 = [i for i in range(10)]\n",
    "ls1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '9', '36', '81']\n"
     ]
    }
   ],
   "source": [
    "ls2 = [str(i**2) for i in range(10) if i%3==0 ]\n",
    "print(ls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e', 'u', 'i', 'a'}\n",
      "['i', 'e', 'u', 'i', 'a', 'a']\n"
     ]
    }
   ],
   "source": [
    "quote = \"life, uh, finds a way\"\n",
    "unique_vowels = {i for i in quote if i in 'aeiou'}\n",
    "vowels = [i for i in quote if i in 'aeiou']\n",
    "print(unique_vowels)\n",
    "print(vowels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Python Libraries`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Libraries**\n",
    "\n",
    "A library (or module, or package) is a Python object with arbitrarily named attributes that you can bind and reference. Simply, a module is a file consisting of Python code. A module can define functions, classes and variables. A module can also include runnable code.\n",
    "\n",
    "You can use any Python source file as a module by executing an import statement in some other Python source file. The import has the following syntax:\n",
    "\n",
    "\n",
    "```\n",
    "import <module name>\n",
    "```\n",
    "\n",
    "By convention it is common to name modules so they can be called by entering an abbreviated name. This is effectively importing the module in the same way that `import <module name>` will do, with the only difference of it being available as ` <module name abbreviation>`. In the case of `numpy`, for example, the abbreviation `np`is used.\n",
    "\n",
    "```\n",
    "import <module name> as <module name abbreviation>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding/Installing Libraries**\n",
    "\n",
    "To add Python libraries to your installation you can use the `conda` package manager that we have installed in the last workshop. Alternatively you can also use the `pip` package manager. The quickest way to do so is via a terminal:\n",
    "\n",
    "* If you are on a **Windows** computer, use the \"Anaconda Command Prompt\" from the Start menu. \n",
    "* On a **Mac**, start up the \"Terminal\". \n",
    "* In **Linux**, use any of the terminals available.\n",
    "\n",
    "\n",
    "The gerneral command syntax is the following:\n",
    "\n",
    "```\n",
    "conda install <package name>\n",
    "```\n",
    "\n",
    "If you are looking for a specific package but are unsure of the exact command line name do a quick google search and/or check the [Anaconda Cloud](https://anaconda.org). Another approach to find the exact name of a package before installation is using the conda search command in terminal; the syntax is the following:\n",
    "\n",
    "```\n",
    "conda search <name>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant Libraries for this course**\n",
    "\n",
    "There is a large variety of open source libraries available in Python. Below is a list of some of the most relevant ones for data science, which will be covered in this course.\n",
    "\n",
    "* Selected data science libraries\n",
    "\n",
    "    * Data Analysis and Processing\n",
    "    >* Pandas (pd)\n",
    "    >* Numpy (np)\n",
    "    * Visualization        \n",
    "    >* matplotlib and pyplot (plt)\n",
    "    >* seaborn (sns)\n",
    "    * Models and methods\n",
    "    >* Scikit Learn (sklearn)\n",
    "    >* statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executing commands from within Jupyter:** For your convenience, it is possible to execute commands from within Jupyter cells using a line prefixed with `!`.\n",
    "In the example, we list (`ls`) the current working directory, and format it for human-readable output (`-h`) and to include more detail (`-l`). Commands are executed in the local user shell and in the conda environment that the notebook uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"ls\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NumPy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\n",
    "\n",
    "- A powerful N-dimensional array object\n",
    "- Sophisticated (broadcasting) functions\n",
    "- Tools for integrating C/C++ and Fortran code\n",
    "- Useful linear algebra, Fourier transform, and random number capabilities\n",
    "\n",
    "Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\n",
    "\n",
    "NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers. In NumPy, dimensions are called axes. The number of axes is rank.\n",
    "\n",
    "In today's short overview tutorial we will cover the following:\n",
    "\n",
    "1. **Creating NumPy Arrays**\n",
    "1. **Modifying (manipulating) NumPy Arrays**\n",
    "1. **NumPy Array Oprations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating NumPy Arrays\n",
    "\n",
    "First, we can use `np.array` to create arrays from python lists. Unlike Python lists, **NumPy is constrained to arrays that all contain the same type**. If types don't match, NumPy will upcast if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1,2,3,4,5])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([3.1, 5, 4, 6])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D =np.array([1,2,'a',3])\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more info in greater details about string types and other datatype objects in the [numpy documentation](https://numpy.org/doc/stable/reference/arrays.dtypes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to explicitly set the data type of the resulting array, we can use the `dtype` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([1,2,3,8],dtype = float)\n",
    "print (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of creating arrays using np functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector of length 5 filled with zeros\n",
    "E = np.zeros(5,dtype = float)\n",
    "\n",
    "# create 2x4 matrix of ones (float)\n",
    "F = np.ones((2,4), dtype= float)\n",
    "\n",
    "# create vector from 0-12 in steps of 2\n",
    "G = np.arange(0,12,2)\n",
    "\n",
    "# create vector from 0 to 1 with five equally (linearly) spaced elements \n",
    "H = np.linspace(0,1,5)\n",
    "\n",
    "# create a 2x2 matrix with random floats in the half-open interval [0.0, 1.0)\n",
    "I = np.random.random((2,2))\n",
    "\n",
    "# return random integers from 0 (inclusive) to 10 (exclusive) of size (4,3,2)\n",
    "J = np.random.randint (0,10,(4,3,2))\n",
    "\n",
    "print(\"E =\", E,\n",
    "      \"\\n\\nF =\", F, \n",
    "      \"\\n\\nG =\", G, \n",
    "      \"\\n\\nH =\", H, \n",
    "      \"\\n\\nI =\", I, \n",
    "      \"\\n\\nJ =\", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying (manipulating) NumPy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data manipulation in Python is nearly synonymous with NumPy array manipulation. We will cover a few categories of basic array manipulations here:\n",
    "- **Attributes of arrays**: Determinig the size, shape, memory consumption and data type of arrays.\n",
    "- **Indexing of arrays**: Getting and setting the value of indivisual array elements.\n",
    "- **Slicing of arrays**: Getting and setting smaller subarrays within a larger array.\n",
    "- **Reshaping of arrays**: Changing the shape of a given array.\n",
    "- **Joining and splitting of arrays**: Combining multiple arrays into one, and splitting one array into many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Array Attributes:\n",
    "In the following some examples on attributes are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns dimension\n",
    "print(\"E ndim: \", E.ndim)\n",
    "\n",
    "# returns shape in form (#row,#col)\n",
    "print(\"F sahpe: \" , F.shape)\n",
    "\n",
    "# returns size (i.e. no of elements)\n",
    "print(\"J size: \", J.size)\n",
    "\n",
    "# returns data type\n",
    "print(\"H dtype: \", H.dtype)\n",
    "\n",
    "# returns length of one array element in bytes\n",
    "print(\"itemsize: \", I.itemsize,\" bytes\")\n",
    "\n",
    "# returns total bytes consumed by the elements of the array\n",
    "print(\"nbytes:  \", I.nbytes, \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Array Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily access single elements as you already know from Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the full array\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 4th element of A is {}\". format(A[3]))\n",
    "print(\"The last element of A is {}\". format(A[-1])) # index from the back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multidimensional array (i.e. a matrix), you access items using a comma-seperated tuple of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember multidimensional array I\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The first element of I is {}\". format(I[0,0]))  #array[row,column]\n",
    "print (\"The last element of I is {}\". format(I[1,1]))   #array[row,column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Array Slicing:\n",
    "\n",
    "**One-dimensional arrays**\n",
    "\n",
    "Just as we can use square brakets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon `:` character. The syntax is as follow:\n",
    "\n",
    "` X[start (incl.):stop (excl.):step]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember array G\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"middle subarray:\", G[2:4])\n",
    "print(\"First 4 elemnts:\", G[:5])\n",
    "print(\"Last 3 elements:\", G[-3:] )\n",
    "print(\"Every other element:\", G[::2])\n",
    "print(\"All elements reversed:\", G[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-dimensional arrays**\n",
    "\n",
    "Multi-dimensional slices work in the same way, with multiple slices seperated by commas. The command is: \n",
    "\n",
    "`X[slice row, slice column]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a multi-dimensional array K\n",
    "K = np.random.randint(0,20, (3,4))\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The first two rows and the first three column: \\n\", K[:2,:3])\n",
    "print(\"All rows and every other column:\\n\", K[:,::2])\n",
    "print(\"All rows and columns reversed:\\n\",K[::-1,::-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Array Reshaping:\n",
    "\n",
    "Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape method. Note that for this to work, the size of the initial array must match the size of the reshaped array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return evenly spaced values within a given interval\n",
    "Y = np.arange(1,25)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re-shape this array into any shape with 24 elements\n",
    "Y.reshape(6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can easily transpose multi-dimensional arrays using `T`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember K\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose K\n",
    "K.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Array Concatenation and Splitting\n",
    "\n",
    "All of the preceding routines worked on single arrays. It's also possible to combine multiple arrays into one, and to conversely split a single array into multiple arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenation of arrays**\n",
    "\n",
    "Concatenation, or joining of two arrays in NumPy, is primarily accomplished using the routine `np.concatenate`. Additionally `np.vstack`, and `np.hstack` may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-dimensional\n",
    "P = np.array([1,2,3])\n",
    "Q = np.array([4,5,6])\n",
    "np.concatenate((P,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-dimensional\n",
    "R = np.array([[3,5,7],[1,3,5]])\n",
    "S = np.array([[2,4,2],[0,9,8]])\n",
    "print(R, \"\\n\")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the axis along which the arrays should be joined\n",
    "print(np.concatenate((R,S), axis = 0), \"\\n\")\n",
    "print(np.concatenate((R,S), axis = 1), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For working with arrays of mixed dimensions, it can be more practical to use the `np.vstack` (vertical stack) and `np.hstack` (horizontal stack) functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack row-wise\n",
    "np.vstack((R,S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack column-wise\n",
    "np.hstack((R,S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting of arrays**\n",
    "\n",
    "The opposite of concatenation is splitting, which is implemented by the functions `np.split`, `np.hsplit`, and `np.vsplit`. For each of these, we can pass a list of indices indicating the split points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the array x into 4 equal subarrays\n",
    "x = np.array([2,4,6,7,8,9,1,3,11,35,55,34])\n",
    "print(np.split(x,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, x4 = np.split(x,4)\n",
    "print(x1, x2, x3, x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multi-dimensional array\n",
    "Z = np.arange(16).reshape((4, 4))\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits array Z into multiple sub-arrays vertically (row-wise)\n",
    "upper, lower = np.vsplit(Z, 2)\n",
    "print(\"upper: \\n\",upper)\n",
    "print(\"lower: \\n\",lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits array Z into multiple sub-arrays horizontally (column-wise)\n",
    "left, right = np.hsplit(Z, 2)\n",
    "print(\"left: \\n\",left)\n",
    "print(\"right: \\n\",right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Array Operations\n",
    "\n",
    "Numpy also allows for **element-wise** as well as linear algebra **matrix-type** operations, which are a key component of scientific computing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two one-dimensional arrays\n",
    "A = np.array([1,2,3,4])\n",
    "B = np.array([9,3,-9,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Element-wise operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise addition\n",
    "C=A+B\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise substraction\n",
    "D=A-B\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise multiplication\n",
    "E=A*B\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise division\n",
    "F=A/B\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two 2x5 matrices\n",
    "M = np.arange(10).reshape(2,5)\n",
    "N = np.random.randint(1,10,10).reshape(2,5)\n",
    "print(M, \"\\n\")\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: performing a matrix multiplication on two 2x5 matrices is not possible\n",
    "M@N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can transpose one of the matrices to obtain a 5x2 matrix, then the operation works\n",
    "M@N.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `Pandas`\n",
    "\n",
    "Pandas is a newer package built on top of `NumPy`, and provides an efficient implementation of a `DataFrame`, the core Pandas object. DataFrames are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs. We will use `Pandas` as the main tool to structure, manipulate and store data throughout this course. As such, Pandas constitutes a core data science library that all of you should be very well familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to `Pandas` objects\n",
    "\n",
    "At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. Pandas provides a host of useful tools, methods, and functionality on top of the basic data structures, but nearly everything that follows will require an understanding of what these structures are. Thus, before we go any further, let's introduce these three fundamental Pandas data structures: the Series, DataFrame, and Index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Pandas Series` Object\n",
    "A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [12,13,14,15,16,17]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = pd.Series(A)\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the output, the Series wraps both a sequence of values and a sequence of indices, which we can access with the values and index attributes. The values are simply a familiar NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is an array-like object of type pd.Index, which we'll discuss in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with a `NumPy` array, data can be accessed by the associated index via the familiar Python square-bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we've seen so far, it may look like the Series object is basically interchangeable with a one-dimensional NumPy array. **The essential difference is the presence of the index**: while the Numpy Array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values.\n",
    "This explicit index definition gives the Series object additional capabilities. For example, the index need not be an integer, but can consist of values of any desired type. For example, if we wish, we can use strings as an index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.Series([0.25, 0.5, 0.75, 1.0],index=['a', 'b', 'c', 'd'])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure which maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations.\n",
    "The Series-as-dictionary analogy can be made even more clear by constructing a Series object directly from a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 2644819,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a dictionary, though, the Series also supports array-style operations such as slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing a dict does not work\n",
    "population_dict['California':'New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing a Series object works\n",
    "population['California':'New York']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Pandas DataFrame` Object\n",
    "\n",
    "The next fundamental structure in `Pandas`  is the `DataFrame` . Like the Series object discussed in the previous section, the DataFrame can be thought of either as a generalization of a NumPy array, or as a specialization of a Python dictionary. We'll now take a look at each of these perspectives.\n",
    "\n",
    "If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by \"aligned\" we mean that they share the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have this along with the population Series from before, we can use a dictionary to construct a single two-dimensional object containing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Series object, the DataFrame has an index attribute that gives access to the index labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the DataFrame has a columns attribute, which is an Index object holding the column labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the DataFrame can be thought of as a generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for accessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Pandas Index` Object\n",
    "\n",
    "We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multi-set, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let's construct an Index from a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.Index([2, 3, 5, 7, 11])\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Index in many ways operates like an array. For example, we can use standard Python indexing notation to retrieve values or slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[::2] # every second object starting at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index objects also have many of the attributes familiar from NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ind.size, ind.shape, ind.ndim, ind.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between Index objects and NumPy arrays is that indices are immutable – that is, they cannot be modified via the normal means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work with index objects\n",
    "ind[1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection in `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection in Series\n",
    "\n",
    "As we saw in the previous section, a Series object acts in many ways like a one-dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us understand the patterns of data indexing and selection in these arrays.\n",
    "\n",
    "Like a dictionary, the Series object provides a mapping from a collection of keys (i.e. the index) to a collection of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C['d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dictionary-like Python expressions and methods to examine the keys/indices and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(C.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add value\n",
    "C['e'] = 1.25\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change value\n",
    "C['a'] = 0\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series builds on this dictionary-like interface and provides array-style item selection via the same basic mechanisms as NumPy arrays – that is, slices, masking, and fancy indexing. Examples of these are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by explicit index (note: including last)\n",
    "C['a':'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by implicit integer index (note: excluding last)\n",
    "C[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[C>=0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking\n",
    "C[(C > 0.3) & (C < 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple indexing\n",
    "C[['a', 'e', \"c\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among these, slicing may be the source of the most confusion. Notice that when slicing with an explicit index (i.e., data['a':'c']), the final index is included in the slice, while when slicing with an implicit index (i.e., data[0:2]), the final index is excluded from the slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexers: loc and iloc**\n",
    "\n",
    "These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as data[1] will use the explicit indices, while a slicing operation like data[1:3] will use the implicit Python-style index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(['a', 'b', 'c', 'd'], index=[1, 3, 5,7])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit index when indexing\n",
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implicit index when slicing\n",
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series.\n",
    "First, the `.loc` attribute allows indexing and slicing that always references the **explicit index**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.iloc` attribute allows indexing and slicing that always references the **implicit Python-style index**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One guiding principle of Python code is that **\"explicit is better than implicit\"**. The explicit nature of `.loc` and `.iloc` make them very useful in maintaining clean and readable code; especially in the case of integer indexes, I recommend using these both to make code easier to read and understand, and to prevent subtle bugs due to the mixed indexing/slicing convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection in DataFrame\n",
    "\n",
    "Recall that a DataFrame acts in many ways like a two-dimensional or structured array, and in other ways like a dictionary of Series structures sharing the same index. These analogies can be helpful to keep in mind as we explore data selection within this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.Series({'California': 423967, 'Texas': 695662,\n",
    "                  'New York': 141297, 'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
    "                 'New York': 19651127, 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual Series that make up the columns of the DataFrame can be accessed via dictionary-style indexing of the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we can also view the DataFrame as an enhanced two-dimensional array. We can examine the raw underlying data array using the values attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this picture in mind, many familiar array-like observations can be done on the DataFrame itself. For example, we can transpose the full DataFrame to swap rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to indexing of DataFrame objects, however, it is clear that the dictionary-style indexing of columns precludes our ability to simply treat it as a NumPy array. In particular, passing a single index to an array accesses a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...while passing a single \"index\" to a DataFrame accesses a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for array-style indexing, we need another convention. Here Pandas again uses the `.loc` and `.iloc` indexers mentioned earlier. Using the `.iloc` indexer, we can index the underlying array as if it is a simple NumPy array (using the implicit Python-style index), but the DataFrame index and column labels are maintained in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, using the `.loc` indexer we can index the underlying data in an array-like style but using the explicit index and column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:'Texas', :'pop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass conditions to the indexer, a technique called **masking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking with .loc\n",
    "data.loc[data.area > 200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking with explicit column index\n",
    "new_df = data[data[\"area\"] > 200000]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masks are conditional statements that are evaluated for every element (i.e., row) of the DataFrame.\n",
    "If you want to combine multiple statements, you can use boolean operators: `&`, `|`. Negation is handled with `~` in pandas masking. Be aware that you cannot use operators such as `and`, `or`, `!`, `not`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data[\"area\"] > 100000) & (data[\"area\"] <= 180000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of these indexing conventions may also be used to set or modify values; this is done in the standard way that you are accustomed to from working with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0, 1] = 90000000\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data anaylsis and manipulation in `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Indexing\n",
    "\n",
    "**Using MultiIndex to create hierarchichal DataFrames**\n",
    "\n",
    "Often it is useful to store higher-dimensional data, i.e. data indexed by more than one or two keys. The most approapriate Pandas function for this is `MultiIndex`. We will demonstrate using a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a 2-D example, first specifying the outer and inner dimensions as well as the values\n",
    "Outer = ['California','California',\n",
    "         'New York','New York',\n",
    "         'Texas','Texas']\n",
    "\n",
    "Inner = [2000,2010,\n",
    "         2000,2010,\n",
    "         2000,2010]\n",
    "\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "\n",
    "# From the outer and inner dimensions we create an index\n",
    "index_list = list(zip(Outer,Inner))\n",
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on this index a multi index can be created\n",
    "index_object = pd.MultiIndex.from_tuples(index_list)\n",
    "index_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate the same result using the `MultiIndex.from_arrays()` function fro **Pandas**. Keep in mind the size of arrays has to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_ind = pd.MultiIndex.from_arrays([Outer, Inner])\n",
    "mul_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we create a data frame with hierarchichal indexes\n",
    "df = pd.DataFrame(index=index_object,\n",
    "                  data=populations,\n",
    "                  columns=[\"Population\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = [164,164, #California \n",
    "        55,55,   #NY State\n",
    "        269,269] #Texas\n",
    "\n",
    "df[\"Area\"] = area\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, we can easily transpose this DataFrame\n",
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MultiIndex as extra dimension**\n",
    "\n",
    "You might notice that we could have stored the same data using a simple DataFrame with index and column labels. The `unstack()` method allows us to convert a hierachically indexed Series into a conventionally indexed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack()[\"Population\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot a level of the (necessarily hierarchical) index labels, returning\n",
    "a DataFrame having a new level of column labels whose inner-most level\n",
    "consists of the pivoted index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack(level = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging data in `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also makes it easy to combine a Series and DataFrame. Multiple Series can be joined to represent a DataFrame, while multiple DataFrame can be joined in a table style join (like in databases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([1,1,2,2,3,3], name = 'Series 1')\n",
    "s2 = pd.Series([1.1, 1.2, 2.2, 2.4, 3.3, 3.6], name = 'Series 2')\n",
    " \n",
    "pd.DataFrame([s1, s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want them to represent columns, we need to supply them as values of a dictionary indexed by column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1s2 = pd.DataFrame({'Series 1': s1, 'Series 2': s2})\n",
    "s1s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge series objects by index\n",
    "pd.merge(s1s2, s1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we join on the index, but we could also join on another column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({\n",
    "    'key': [1,2,4,5,5,6],\n",
    "    'value': [0.1, 0.2, 0.4, 0.5, 0.55, 0.6]\n",
    "})\n",
    "\n",
    "b = pd.DataFrame({\n",
    "    'key': [1,2,4,5,5,6],\n",
    "    'value': pd.Series([0.1, 0.2, 0.4, 0.5, 0.55, 0.6]) ** 2\n",
    "})\n",
    "\n",
    "pd.merge(a, b, on = 'key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there are more rows than in each of the individual DataFrames. This is because we do an inner style join, where we return every match, and the key `5` matches four times, as it is present two times in both DataFrames!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick data analysis in `Pandas`\n",
    "\n",
    "In practice your dataset will often be considerably larger than the illustrative example above. In these cases it is often useful to run brief descriptive statistical analyses on the set, which will help to get a first feel for the data. \n",
    "\n",
    "We will demonstrate how to do this using the provided `iris.csv` dataset. This is a famous dataset used frequently for educational purposes in data science. You can read up on the dataset, its content and its origins here: https://en.wikipedia.org/wiki/Iris_flower_data_set.\n",
    "\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The __three classes__ in the Iris dataset:\n",
    "* Iris-setosa (n=50)\n",
    "* Iris-versicolor (n=50)\n",
    "* Iris-virginica (n=50)\n",
    "\n",
    "The __four features__ of the Iris dataset:\n",
    "* sepal length in cm\n",
    "* sepal width in cm\n",
    "* petal length in cm\n",
    "* petal width in cm\n",
    "\n",
    "<img src=\"./iris.original.png\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first read in the data as a dataframe\n",
    "# you can optionally set the index by using the index_col function\n",
    "Iris_set = pd.read_csv(\"../data/iris.csv\", index_col=\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first five columns\n",
    "Iris_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the last 5 columns\n",
    "Iris_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the describe function provides an overview of key descriptive statistics by columns\n",
    "Iris_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info on data types and counts\n",
    "Iris_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also focus your analysis on individual features by indexing them\n",
    "Iris_set[\"Sepal.Width\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally you can call the functions seperately, e.g. max\n",
    "Iris_set.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing numerical data (`NaN` values)\n",
    "If you have paid attention to the descriptive statistics above you will have noticed that the __counts__ of values differ across features. This is a first indication for missing numerical data. In the real world you will often encounter incomplete and noisy data which will require pre-processing before you can apply data science and machine learning tools to them. In this section we will provide a quick overview on how to deal with missing data in your datasets. \n",
    "Note that there are multiple ways how Python, Pandas, and other packages might highlight missing data:\n",
    "* `NaN` - (acronym for Not a Number), is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation. Pandas uses this for display\n",
    "* `pd.NA` - is a special pandas type, that is shorthand for all other types\n",
    "* `None` - Python-specific object that is often used for missing data in Python code. Because it is a Python object, `None` can only be used in arrays with data type 'object' (i.e., arrays of Python objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detecting missing numerical data**\n",
    "\n",
    "Pandas data structures have two useful methods for detecting null data: `isnull()` and `notnull()`. Either one will return a Boolean mask over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnull() returns \"True\" for every missing numerical value in the dataset\n",
    "Iris_set.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notnull() returns \"False\" for every missing numerical value in the dataset\n",
    "Iris_set.notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing numerical data\n",
    "In essence two approaches to dealing with missing data exist:\n",
    "* __Elimination__: Dropping null values from the dataset\n",
    "* __Imputation__: Imputing/replacing null values with numerical values/estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elimination (i.e. dropping null values)**\n",
    "\n",
    "We cannot drop single values from a data frame; we can only drop full rows or full columns. Depending on the application, you might want one or the other, so `dropna()` gives a number of options:\n",
    "\n",
    "We use the `dropna(axis=0)` function to drop all rows from the dataset that incl. null values. Note that `dropna()` will not modify your dataframe unless you call `dropna(axis=0,inplace=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.dropna(axis=0, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the dataset is unchanged\n",
    "Iris_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `dropna(axis=1)` function to drop all columns from the dataset that incl. null values. Note that, also in this case, `dropna()` will not modify your dataframe unless you call `dropna(axis=1,inplace=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, note that dropna() does not modify the original dataframe!\n",
    "len(Iris_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can therefore also use dropna() to easily identify the number of rows with missing values \n",
    "len(Iris_set)-len(Iris_set.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you whish to clean the data with dropna it is good practice to define a new data frame\n",
    "Iris_set_clean = Iris_set.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Iris_set_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputation (i.e. filling null values)**\n",
    "\n",
    "Imputation fills null values with numerical values chosen by the data scientist. For this `fillna()`provides the appropriate tools.\n",
    "\n",
    "The data scientist will usually choose one of the following methods:\n",
    "* Fill with zeros: `fillna(0)`\n",
    "* Conduct a forward fill (i.e. filling NaN values with data from following observation): `fillna(method=\"ffill\")`\n",
    "* Conduct a backward fill (i.e. filling NaN values with data from previous observation): `fillna(method=\"bfill\")`\n",
    "* Fill with the column mean,max,min, etc.: `df[\"Col_name\"].fillna(value=df[\"Col_name\"].mean())`\n",
    "* Custom fill depending on the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna() allows for inserting a number of choice, confirm with inplace=True\n",
    "Iris_set[[\"Sepal.Length\"]].fillna(value=Iris_set[\"Sepal.Length\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data aggregation and grouping\n",
    "The `groupby()` method allows for grouping of rows and the application of aggregation functions to these grouped rows. It is a highly popular method in data science used extensively. We will return to the iris dataset for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might want to group rows according to \"Species\" and assign to a new variable \"Species_group\"\n",
    "Species_groups = Iris_set.groupby(\"Species\").describe()\n",
    "Species_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might select an individual group across all features\n",
    "# note that you need to transpose the array before you can index the group\n",
    "Iris_set.groupby(\"Species\").describe().transpose()[[\"versicolor\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply aggregation functions to each group using groupby-apply syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.groupby(\"Species\").max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we applied the `max` aggregation function to all groups, but we can restrict that to columns and even use different aggregation functions per column of interest using the more flexible `agg` functionality. We need to give the desired aggregation function for every column, and columns we don't explicitly list get omitted in aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.groupby(\"Species\").agg({'Sepal.Length': 'min', 'Sepal.Width': 'max', 'Petal.Length': 'median'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check which functions are available by looking at the [Documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-aggregation-methods).\n",
    "\n",
    "Sometimes you might wish to give the resulting columns names that reflect what aggregation you used. You can use named aggregation for that, but must then use keyword arguments instead of passing a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'group': ['A', 'A', 'B', 'C', 'C', 'C'],\n",
    "    'somevalue': [1, 2, 1.5, 1, 2, 3],\n",
    "    'anothervalue': [1.1, 2.1, 1.6, 1.1, 2.1, 3.1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group').agg(\n",
    "    somevalue_avg = pd.NamedAgg(column='somevalue', aggfunc='mean'), \n",
    "    anothervalue_max = pd.NamedAgg(column='anothervalue', aggfunc='max'),     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unluckily, this only works if the new column names are valid Python keywords, which would for example not be the case when we want to name resulting columns in our flower example using the syntax `agg.columnname`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.groupby(\"Species\").agg(\n",
    "    avg.Sepal.Length = pd.NamedAgg(column='Sepal.Length', aggfunc='mean'),\n",
    "    median.Sepal.Length = pd.NamedAgg(column='Sepal.Length', aggfunc='median')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need a little trick (which is presented in the pandas documentation [here](https://pandas.pydata.org/docs/user_guide/groupby.html#named-aggregation)): Python functions accept a dynamic unpacking of dictionaries for keyword arguments (`**kwargs`), which is why we can circumvent the illegal named aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris_set.groupby(\"Species\").agg(\n",
    "    **{\n",
    "    'avg.Sepal.Length': pd.NamedAgg(column='Sepal.Length', aggfunc='mean'),\n",
    "    'median.Sepal.Length': pd.NamedAgg(column='Sepal.Length', aggfunc='median')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final note**:\n",
    "To add a table of content to your jupyter notebook, you need to install the `nbextension` available on https://github.com/minrk/ipython_extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
