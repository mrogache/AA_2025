{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 09` - Decision Trees\n",
    "\n",
    "In this workshop we get to know tree-based methods, which are commonly used in a myriad of classification and regression problems, and look at how to combine methods for ensembles.\n",
    "\n",
    "We will cover the following: \n",
    "1. Decision Trees for classification (breast cancer example)\n",
    "1. Decision Trees for regression (peak electrical power example)\n",
    "1. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Decision Trees (DTs)** are a non-parametric supervised learning method used for *classification* and *regression*. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the features.\n",
    "\n",
    "Some **advantages** of decision trees are:\n",
    "\n",
    "- Trees are simple to understand and to interpret. Trees can be visualised.\n",
    "\n",
    "- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note, however, that missing values do need to be handled.\n",
    "\n",
    "- Trees are able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. \n",
    "\n",
    "- Trees are able to handle multi-output problems.\n",
    "\n",
    "- Trees are explainable. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in an opaque model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "\n",
    "- It is possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "\n",
    "- Trees perform well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "\n",
    "The **disadvantages** of decision trees include:\n",
    "\n",
    "- DT learners can create over-complex trees that do not generalise well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node, or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "- DT learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees for Classification: Classifying Breast Cancer Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col=\"id\")\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To abstract from the relatively high-dimensionality of the breast cancer dataset let us confine our analysis to a two-dimensional feature vector consisting of `area_mean` and `concave points_mean` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cells():\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')\n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "    plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "    plt.legend(['Malignant','Benign'],fontsize=12)\n",
    "    \n",
    "plot_cells()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define X and Y vectors correspondingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# recode Y to 0 and 1\n",
    "Y  = np.where(Y==\"M\", int(1), Y) \n",
    "Y  = np.where(Y==\"B\", int(0), Y) \n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not need to scale, as Decision Trees do not work based on distances across features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify and fit a simple `DecisionTreeClassifier`, which is available via `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(max_depth=3,criterion='gini') # we set gini as our impurity measure\n",
    "tree_classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision estimator has an attribute called `tree_`  which stores the entire tree structure and allows access to low-level attributes. The binary `tree_` attribute is represented as a number of parallel arrays. The i-th element of each array holds information about the node `i`. Node 0 is the tree's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = tree_classifier.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among those arrays, we have:\n",
    "\n",
    "   - left_child: id of the left child of the node\n",
    "   - right_child: id of the right child of the node\n",
    "   - feature: feature used for splitting the node\n",
    "   - threshold: threshold value used for splitting the node\n",
    "   - impurity: the impurity at the node\n",
    "   - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign various tree attributes\n",
    "n_nodes = structure.node_count\n",
    "n_leaves = structure.n_leaves\n",
    "children_left = structure.children_left\n",
    "children_right = structure.children_right\n",
    "feature = structure.feature\n",
    "threshold = structure.threshold\n",
    "impurity = structure.impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num nodes: \\t\",n_nodes)\n",
    "print(\"Num leaves: \\t\",n_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"left children per node: \", children_left)\n",
    "print(\"right children per node: \", children_right)\n",
    "print(\"Decision feature at node: \", feature)\n",
    "print(\"Threshold of feature at node\", threshold)\n",
    "print(\"Impurity at node: \", impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method can be used to get the index of the leaf that each sample is predicted as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_ids = tree_classifier.apply(X)\n",
    "\n",
    "leaf_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's retrieve the decision path of a selected sample. \n",
    "\n",
    "The `decision_path` method allows us to retrieve the node indicator functions. A non-zero element of an indicator matrix at the position (i, j) indicates that the sample i goes through the node j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indicators = tree_classifier.decision_path(X)\n",
    "\n",
    "# let's generate a random sample ID\n",
    "sample_id = np.random.randint(0,len(X))\n",
    "\n",
    "# retrieve decision_path for that sample\n",
    "node_index = node_indicators.indices[node_indicators.indptr[sample_id]: #indptr maps the elements of data and indices to the rows of the sparse matrix\n",
    "                                    node_indicators.indptr[sample_id + 1]]  #indptr maps the elements of data and indices to the rows of the sparse matrix\n",
    "\n",
    "print(\"Decision path for sample \" + str(sample_id), \": \", str(node_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision path for sample %s: %s' % (str(sample_id), str(node_index)))\n",
    "print('Feature values of sample %s: %s \\n' % (sample_id, X[sample_id]))\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    # skip leaf node\n",
    "    if leaf_ids[sample_id] == node_id:\n",
    "        continue\n",
    "    \n",
    "    # for all other nodes, retrieve the feature values\n",
    "    if (X[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"Decision at node %s: value for feature %s (%s) is %s the threshold of %s\"\n",
    "          % (node_id,\n",
    "             feature[node_id],\n",
    "             X[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably this is a little abstract and does not seem very intuitive. Therefore, we will visualize the underlying tree and try to relate it back to the tree structure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "def plot_class_tree_sklearn(tree_depth):\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth=tree_depth,\n",
    "                                  criterion='gini') # we set gini as our impurity measure\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    dot_data = export_graphviz(model,\n",
    "                               feature_names=['area_mean','concave points_mean'],\n",
    "                              class_names = [\"Begning\",\"Malignant\"],out_file=None)  \n",
    "    \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_tree_sklearn(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decision tree each node is represented by a box. For each node the following information is provided:\n",
    "- decision feature and threshold\n",
    "- impurity\n",
    "- number of samples\n",
    "- number of samples per class\n",
    "- class (i.e., majority vote)\n",
    "\n",
    "We can, thus, easily relate this back to the tree attributes we computed above. A selection is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num nodes: \\t\",n_nodes)\n",
    "print(\"Num leafs: \\t\",n_leaves)\n",
    "print(\"Feature at node\", feature) # -2 indicates no feature/threshold, i.e. a leaf\n",
    "print(\"Threshold of feature at node\", threshold)\n",
    "print(\"Impurity at node: \", impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision surfaces\n",
    "\n",
    "As we have seen in the lecture, another intuitive representation of decision trees is the use of decision surfaces. These can be related back directly to the decision tree. For ease of use, a plotting routine has been prepared that combines fitting and plotting into a single routine and allows for easy adjustment of tree depth and the minimum samples per leaf (discussed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_surface(tree_depth):\n",
    "    \n",
    "    # specify and fit decision tree classifier\n",
    "    #from sklearn.tree import DecisionTreeClassifier, export_graphviz # we also call the garphviz module for later visualization\n",
    "    model = DecisionTreeClassifier(max_depth=tree_depth,\n",
    "                                  criterion='gini') # we set entropy as our impurity measure\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    # get tree attributes\n",
    "    \n",
    "    attributes = model.tree_\n",
    "    \n",
    "    # define range per feature\n",
    "    x_range = [0,2600] # i.e. mean area\n",
    "    y_range = [0, 0.21] # i.e mean conc. points\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    \n",
    "    # plot classifcation regions\n",
    "    grid=1000\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "\n",
    "\n",
    "    zz = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    zz = zz.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy,zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "    \n",
    "    # plot datapoints\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "    plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "    plt.legend([s1,s2],['Malignant','Benign'],fontsize=12)\n",
    "    \n",
    "    print(\"number of nodes: \", attributes.node_count)\n",
    "    print(\"number of leafs: \", attributes.n_leaves)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #plt.savefig(\"Breast_Cancer_Decision_Surface_{}depth.pdf\".format(tree_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling overfitting in Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision-tree learners can create overly complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can easily be seen by increasing tree depth to unreasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do about overfitting in sklearn? As mentioned, we have several tools at our disposal:\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. A too high value of maximum depth causes overfitting, whereas a too low value causes underfitting.\n",
    "- **min_samples_leaf**: By specifying a minimum number of samples per leaf, overfitting can be controlled for.\n",
    "- **ccp_alpha**: Cost Complexity (CCP) alpha paramter determining the size of the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the **cost complexity** as an effective measure in avoiding overfitting. The cost complexity of a tree (CCP(T)) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "CCP(T) = ERR(Z) + \\alpha L(T)\n",
    "\\end{equation}\n",
    "\n",
    "where ERR(Z) is the total misclassification rate of the terminal nodes and L(T) is the number of terminal nodes of tree T. This type of formula should look familiar, as it closely resembles the regularized regression loss functions we know.\n",
    "\n",
    "To get an idea of what values of $\\alpha$ could be appropriate, `scikit-learn` provides `DecisionTreeClassifier.cost_complexity_pruning_path` that returns the effective alphas (i.e., those that will achieve the next step in complexity reduction) and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "# fit decision tree (without limit on max_depth, i.e. tree will grow fully if alpha is set to 0)\n",
    "tree_classifier = DecisionTreeClassifier(random_state=0, \n",
    "                                         criterion=\"gini\")\n",
    "\n",
    "# compute cost_complexity_pruning_path \n",
    "path = tree_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return path\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost_complexity_pruning_path\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")  # we remove the last alpha as this corresponds to a tree with only the root node\n",
    "ax.set_xlabel(\"effective alpha\",fontsize=16)\n",
    "ax.set_ylabel(\"total impurity of leaves\",fontsize=16)\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\",fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a decision tree using the effective alphas. The last value in ccp_alphas is the alpha value that prunes the whole tree, leaving the tree with one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    tree.fit(X_train, y_train)\n",
    "    trees.append(tree)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      trees[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of this example, we remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = trees[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [tree.tree_.node_count for tree in trees]\n",
    "depth = [tree.tree_.max_depth for tree in trees]\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\",fontsize=16)\n",
    "ax[0].set_ylabel(\"number of nodes\",fontsize=16)\n",
    "ax[0].set_title(\"Number of nodes vs alpha\",fontsize=16)\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\",fontsize=16)\n",
    "ax[1].set_ylabel(\"depth of tree\",fontsize=16)\n",
    "ax[1].set_title(\"Depth vs alpha\",fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ccp_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could implement a grid search over the identified effective alphas to determine where predictive performance is maximized (see coding challenge for this workshop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees for Regression: Predicting Peak Electricity Demand\n",
    "\n",
    "We continue with our electric power example from last week which we retieved from PJM from the following link [here](https://dataminer2.pjm.com/feed/hrl_load_metered/definition). The files we are loading are the raw files we downloaded from this source. The final input data for our code is `Pittsburgh_load_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../data/Pittsburgh_load_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y\")\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda x: x.month)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y vectors\n",
    "Xp = df[\"High_temp\"].values\n",
    "Yp = df[\"MAX\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(Xp, Yp, marker=\"x\")\n",
    "plt.xlabel(\"High Temperature (°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will unse the `DecisionTreeRegressor` class in `scikit-learn` to fit and plot a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "\n",
    "def plot_tree_regression_line(tree_depth):\n",
    "\n",
    "    # fit regression model (to full data)\n",
    "    Tree_reg = DecisionTreeRegressor(max_depth=tree_depth,\n",
    "                                    criterion=\"squared_error\")\n",
    "    Tree_reg.fit(Xp.reshape((-1,1)), Yp)\n",
    "    Y_pred = Tree_reg.predict(Xp.reshape((-1,1)))\n",
    "    \n",
    "    attributes = Tree_reg.tree_\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(Xp, Yp, marker=\"x\")\n",
    "    plt.plot(np.arange(-18,40,1), Tree_reg.predict(np.arange(-18,40,1).reshape((-1,1))), marker=\"x\", color='C1')\n",
    "    plt.xlabel(\"High Temperature (°C)\", fontsize=16)\n",
    "    plt.ylabel(\"Peak Demand (GW)\", fontsize=16)\n",
    "    \n",
    "    print(\"number of nodes: \", attributes.node_count)\n",
    "    print(\"number of leafs: \", attributes.n_leaves)\n",
    "    print(\"R^2: \", r2_score(Yp, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_regression_line(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "def plot_regression_tree_sklearn(tree_depth):\n",
    "\n",
    "    # fit regression model (to full data)\n",
    "    model = DecisionTreeRegressor(max_depth=tree_depth)\n",
    "    model.fit(Xp.reshape((-1,1)), Yp) \n",
    "    \n",
    "    dot_data = export_graphviz(model,\n",
    "                              feature_names=[\"High_temp\"])  \n",
    "    \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_tree_sklearn(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "In predictive modeling, “risk” is equivalent to variation (i.e. variance) in prediction error. Ensemble methods are targeted at reducing variance, thus increasing predictive power.\n",
    "The core idea is that by combining the outcomes of individual models, e.g., by taking an average, variance may be reduced. Thus, using an average of two or more predictions can potentially lead to smaller error variance, and therefore better predictive power.\n",
    "\n",
    "We will discuss three forms of ensemble learning:\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Random Forests\n",
    "\n",
    "Let's start applying the above method on the classification problem of the *breast cancer* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col=\"id\")\n",
    "cancer_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "Y  = np.where(Y==\"M\", int(1), int(0))\n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split on breast cancer dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "The goal in boosting is to directly improve areas in the data where our model makes errors by forcing the model to pay more attention to those records. \n",
    "\n",
    "Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models misclassified. In some cases, boosting has been shown to yield better accuracy than bagging (see next section), but it also tends to be more likely to overfit the training data. By far the most common implementation of boosting is Adaboost, although some newer algorithms are reported to achieve better results.\n",
    "\n",
    "In boosting, an equal weight (uniform probability distribution) is given to the sample training data (say D1) at the starting round. This data (D1) is then given to a base learner (say L1). The misclassified instances by L1 are assigned a weight higher than the correctly classified instances, but keeping in mind that the total probability distribution will be equal to 1. This boosted data (say D2) is then given to second base learner (say L2) and so on. The results are then combined in the form of voting.\n",
    "The steps in boosting are:\n",
    "\n",
    "1. Fit a model to the data.\n",
    "2. Draw a sample from the data so that misclassified records (or records with large prediction errors) have higher probabilities of selection.\n",
    "3. Fit the model to the new sample.\n",
    "4. Repeat Steps 2–3 multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test) # return the mean accuracy on the given test data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. If a small change in the prediction for a case causes no change in error, then the next target outcome of the case is zero.\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage, ``n_classes_`` trees are fitted on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=1000)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost is an ensemple method that uses boosting. While XGBoost is not included in sklearn, there is a very well developed API that can interface with sklearn.\n",
    "We assume you have the appropriate package installed (if you are using the `environment.yml`, you should be set.)\n",
    "For further details refere to\n",
    "- XGBoost: see [here](https://xgboost.readthedocs.io/en/latest/)\n",
    "\n",
    "Let us fit a simple classifier to the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sepcify and fit model\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(booster=\"gbtree\")\n",
    "xgb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the usual sklearn estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "confusion_matrix(y_test,xgb_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,xgb_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there is likely room for improvement as you grid search some of the hyperparameters. However, by just taking the default setting, we already achieve an accuracy score that is comparable to that of a grid-searched decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap aggregating, often abbreviated as bagging, involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy.\n",
    "\n",
    "In bagging the samples are generated in such a way that the samples are different from each other, but replacement is allowed. Replacement means that an instance can occur in multiple samples multiple times or it can not appear in some samples at all. These samples are then given to multiple learners and the results from each learner are combined in the form of voting. Bagging comprises two steps:\n",
    "1. Generate multiple random samples (by sampling with replacement from the original data) — this method is called “bootstrap sampling.”\n",
    "2. Running an algorithm on each sample and producing scores.\n",
    "\n",
    "This technique is very useful for algorithms such as trees and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model = BaggingClassifier(estimator=\n",
    "    tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2, random_state=1), \n",
    "    max_samples=0.5, \n",
    "    max_features=0.5, random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random Forests is a selection of n trees which are trained in parallel. Predictions are made by averaging the outputs across these n trees. Random Forest are most often combined with **bagging**, i.e. different boostrap samples of the training data are used to train the individual trees.\n",
    "\n",
    "Further details can be found in the following links [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sepcify and fit model\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=10000, \n",
    "                                       bootstrap=True) # we select boostrapp, i.e. we use bagging\n",
    "rf_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "confusion_matrix(y_test,rf_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,rf_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just by taking the default setting, we obtain very good results that are comparable to those of the fully grid-searched decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
