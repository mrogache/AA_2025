{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 11` - Clustering\n",
    "\n",
    "This workshop deals with `unsupervised learning`. In unsupervised learning, we are not trying to predict a continuous value (regression) or a discrete value/class (classification) based on a set of features, but instead we are trying to find (hidden) structures in the data. Therefore, there is no exact measure to tell if your algorithm works well or does not (because we have no fixed reference (such as a label) to compare our results to). During this workshop, we will use clustering algorithms on datasets for which we __do__ have labels, just to give you a better intuition. In real world examples, however, assessing the quality of your clustering result is __more difficult__!\n",
    "\n",
    "We will focus on hard clustering and review the following algorithms:\n",
    "1. Partitioning clustering: k-means\n",
    "1. Hierarchichal clustering\n",
    "1. Short excursion to soft clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Iris\n",
    "For a first intuition, we will be working on the iris dataset again, which you all are familiar with by now. As you remember, in reality there are __three__ different species of irises in the dataset. So, let's see whether we are able to confirm this number of __clusters__ in our dataset using unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('../data/iris.csv', index_col=\"number\").dropna(axis=0)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this an unsupervised learning task we need to drop the label (i.e. `Species`). For later checks we save the response as variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop(\"Species\", axis=1)\n",
    "y = iris[\"Species\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep and Scaling\n",
    "First, let's start out by scaling the data. The K-Means algorithm (and any other clustering algorithm for that matter) minimizes some intra-cluster distance (while maximizing inter-cluster distances). For the case of k-means this is typically defined as the **euclidean distance** from the midpoint of each cluster to all points in this cluster. Other distance metrics are also sometimes used (e.g., **Manhattan distance**)\n",
    "\n",
    "If one feature has a bigger spread than others it will be more important than other factors for the outcome of the clustering. We will revisit this later! For now, let's standardize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# create a df out of the array\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_scaled = X_scaled_df\n",
    "iris_scaled[\"Species\"] = iris[\"Species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical patterns remain in the scaled data, just at a different scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=iris_scaled, hue=\"Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning case (kNN classification)\n",
    "\n",
    "Before we start with the actual clustering, let's revisit the classification methods we talked about during past workshops and see how well they are doing in terms of classification error. For that, we are importing a `kNN classifier` from the sklearn library and have a look at the confusion matrix and the classification report.\n",
    "\n",
    "Remember that kNN has some intuitive parallels to k-means clustering (see lecture material)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are not doing a train-test-split or other advanced methods here; this is not a valid approach for a real supervised learning projects!\n",
    "model_neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "model_neigh.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_neigh.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the performance of our classifier we check relevant test metrics, which you are familiar with by now. We use the `classification_report` class to return a neat summary of the most important classification metrics. We also review the previously discussed `confusion_matrix`.\n",
    "\n",
    "A quick reminder:\n",
    "- The **precision** is the ratio `tp / (tp + fp)` where tp is the number of true positives and fp the number of false positives. The precision captures the ability of the classifier not to label a negative sample as positive.\n",
    "- The **recall** is the ratio `tp / (tp + fn)` where tp is the number of true positives and fn the number of false negatives. The recall captures the ability of the classifier to find all the positive samples.\n",
    "- The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "- The F-beta score weights recall more than precision by a factor of `beta`. `beta == 1.0` means recall and precision are equally important.\n",
    "- The support is the number of occurrences of each class in `y_true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"SpeciesPred\"] = y_pred\n",
    "print(classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning Case (k-means)\n",
    "Now, let's drop the labels and just have a look at the given features. How would we group the datapoints for the different observed flowers?\n",
    "In a typical unsupervised learning case we normally just have a rough idea of how many clusters to expect.\n",
    "\n",
    "Three approaches are commonly applied:\n",
    "1. Use expert knowledge\n",
    "1. Plot residual loss for different numbers of clusters, find 'elbow' and select corresponding number of clusters\n",
    "1. Use hierarchical clustering to detect suitable branching and corresponding number of clusters\n",
    "\n",
    "We will focus on the residual loss (elbow method) as a selection criterion for now. In practice, a combination of all three methods is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 20  # we have 147 datapoints, more than 20 clusters are definitely not reasonable!\n",
    "\n",
    "clusters = []\n",
    "losses = []\n",
    "\n",
    "for k in range(k_max):\n",
    "    model = KMeans(n_clusters=k+1) # initialize\n",
    "    model.fit(X_scaled) # fit\n",
    "    clusters.append(k+1)\n",
    "    losses.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters, losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.xticks(range(k_max+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters, losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.xlim([0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters, losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.xlim([0,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we would expect a good amount of clusters to lie in the region of two to five. Of course, we know that the correct answer is 3 (i.e., one per each species). However, in a true unsupervised learning setting there is no frame of reference and we'd need to draw on several indicators (quantitative and qualitative) to settle on a final number of clusters. For illustrative purposes let's select k=2, the minimum sensible choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-fit algorithm\n",
    "two_means = KMeans(n_clusters=2)\n",
    "two_means.fit(X_scaled)\n",
    "\n",
    "# match records to clusters by calling predict\n",
    "two_means.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"zero\", \"one\", \"two\", \"three\",\"four\",\"five\"]\n",
    "iris_scaled[\"two\"] = two_means.predict(X_scaled)\n",
    "iris_scaled[\"two\"] = iris_scaled[\"two\"].apply(lambda x: numbers[x])\n",
    "sns.pairplot(data=iris_scaled, hue=\"two\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this for the \"correct\" number of cluster, i.e., three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_means = KMeans(n_clusters=3, n_init='auto')\n",
    "three_means.fit(X_scaled)\n",
    "iris_scaled[\"three\"] = three_means.predict(X_scaled)\n",
    "iris_scaled[\"three\"] = iris_scaled[\"three\"].apply(lambda x: numbers[x])\n",
    "sns.pairplot(data=iris_scaled, hue=\"three\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: In a real-world case (i.e., one where no labels are available), this would be the point where you would try to characterize your identified clusters. By examining visualizations and descriptive statistics, you would aim to understand _why_ the clusters differ and _which features_ are responsible for those differences. In other words, clusters must be _interpreted_, not just _computed_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would k=5 fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_means = KMeans(n_clusters=5)\n",
    "five_means.fit(X_scaled)\n",
    "iris_scaled[\"five\"] = five_means.predict(X_scaled)\n",
    "iris_scaled[\"five\"] = iris_scaled[\"five\"].apply(lambda x: numbers[x])\n",
    "sns.pairplot(data=iris_scaled, hue=\"five\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From inspection, we can tell that k=5 achieves relatively poorer inter-cluster separation (e.g., 1 and 3 are very similar). From these analyses, a data scientist would most likely select 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement a second very neat way of evaluating clusters - a silhouette score analysis. The silhouette score is a measure of inter- and intra-cluster distance and can range from -1 (indicating incorrect cluster assignment) to 1 (indicating perfect cluster assignment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # 1st subplot is the silhouette plot\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # initialize k-means with n_clusters value\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    # silhouette_score gives the average value for all the samples.\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # compute silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # aggregate silhouette scores for samples belonging to cluster i and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # label silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # compute new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette plot for various clusters\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # 2nd plot showing actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_scaled[:, 0], X_scaled[:, 1], marker=\".\", s=200, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # labeling clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"Visualization of clustered data\")\n",
    "    ax2.set_xlabel(\"Sepal length\")\n",
    "    ax2.set_ylabel(\"Sepal width\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on iris dataset with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Unlike K-Means, hierarchical clustering defines step-wise decision rules for how to seperate the data into clusters. This has the advantage that it is more resembling the human decision making process and is, therefore, very intuitive for stakeholders to understand. \n",
    "\n",
    "\n",
    "Let's have a look at how a supervised decision tree would look like for our iris dataset and then back this up with an unsupervised hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning Case (decision tree)\n",
    "\n",
    "The supervised equivalent of hierarchichal clustering is decision tree classification, which we will briefly re-visit in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# we engineer a new feature here to enhance performance\n",
    "X[\"Petal.Area\"] = X[\"Petal.Length\"]*X[\"Petal.Width\"]\n",
    "tree.fit(X, y)\n",
    "\n",
    "y_pred_tree = tree.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y, y_pred=y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(tree, out_file=None, \n",
    "                           class_names=iris[\"Species\"].unique(), \n",
    "                           feature_names=X.columns)  \n",
    "\n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning Case (agglomerative clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=3) # the number of clusters to find\n",
    "y_pred_agglo = agglo.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(agglo, labels=agglo.labels_)\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at how the three clusters resulting from agglomerative clustering compare to the actual class memberships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"Agglo\"] = y_pred_agglo\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(9,5))\n",
    "sns.scatterplot(ax=ax[0], x=\"Sepal.Length\", y=\"Petal.Width\", data=iris, hue=\"Agglo\")\n",
    "sns.scatterplot(ax=ax[1], x=\"Sepal.Length\", y=\"Petal.Width\", hue=\"Species\", data=iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Excursion to Soft Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not the focus of this notebook, we want to show you how you can apply soft clustering using a toy example. Because scikit-learn does not implement the soft clustering technique we talked about in the lecture, we will use a package that implements **fuzzy c-means** but retains a scikit-learn compatible API: _skfda_ (https://fda.readthedocs.io/en/stable/modules/ml/autosummary/skfda.ml.clustering.FuzzyCMeans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create some data using sklearn's `make_blobs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "C = 3\n",
    "\n",
    "X, y_true = make_blobs(n_samples=N, centers=C, cluster_std=3, random_state=42)\n",
    "X = X[:, ::-1]\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to preprocess the data as `skfda` works with data grids, but afterwards we can use the usual API of sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.ml.clustering import FuzzyCMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process for this specific module\n",
    "X_f = skfda.FDataGrid(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fuzzifier` argument here resembles the degree of fuzziness $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_model = FuzzyCMeans(n_clusters=3, fuzzifier = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = fuzzy_model.fit_predict(X_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model exposes the degrees of membership as an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = fuzzy_model.membership_degree_\n",
    "U_max = np.max(U, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to visualize clusters (style) and degree of membership (color) in one plot to show (un)certainty. Feel free to experiment around with the fuzziness parameter and see how it changes the plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    x=X[:,0], \n",
    "    y=X[:,1], \n",
    "    style=pd.Series(cluster, name='Cluster'), \n",
    "    hue=pd.Series(U_max**2, name='$u_{ik}$')\n",
    ")\n",
    "ax.legend(loc = 'center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
