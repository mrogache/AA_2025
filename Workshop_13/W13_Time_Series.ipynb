{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 13` - Time Series Analysis\n",
    "\n",
    "In this notebook, we provide a short introduction to time series analysis in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "A univariate time series is a sequence of measurements of the same variable collected over time.  Most often, the measurements are made at regular time intervals.\n",
    "\n",
    "One difference from standard linear regression is that the data are not necessarily independent and not necessarily identically distributed. One defining characteristic of time series is that this is a list of observations where the ordering matters. Ordering is very important because there is dependency and changing the order could change the meaning of the data.\n",
    "\n",
    "**Basic Objectives of the Analysis**\n",
    "\n",
    "The fundamental objective usually is to determine a model that describes the pattern of the time series. Use cases for such a model are:\n",
    "\n",
    "- to describe the important features of the time series pattern.\n",
    "- to explain how the past affects the future or how two time series can “interact”.\n",
    "- to forecast future values of the series.\n",
    "- to possibly serve as a control standard for a variable that measures the quality of product in some manufacturing situations.\n",
    "\n",
    "**Types of Models**\n",
    "\n",
    "There are two basic types of “time domain” models.\n",
    "\n",
    "1. Models that relate the present value of a series to past values and past prediction errors - these are called **ARIMA** models (for Autoregressive Integrated Moving Average).\n",
    "\n",
    "2. Ordinary regression models that use time indices as x-variables. These can be helpful for an initial description of the data and form the basis of several simple forecasting methods.\n",
    "\n",
    "**Some important questions to consider when first looking at a time series are:**\n",
    "\n",
    "- Is there a trend, meaning that, on average, the measurements tend to increase (or decrease) over time?\n",
    "- Is there seasonality, meaning that there is a regularly repeating pattern of highs and lows related to calendar time such as seasons, quarters, months, days of the week, and so on?\n",
    "- Are their outliers? In a regression, outliers are far away from your line. With time series data, your outliers are far away from your other data.\n",
    "- Is there a long-run cycle or period unrelated to seasonality factors?\n",
    "- Is there constant variance over time, or is the variance non-constant?\n",
    "- Are there any abrupt changes to either the level of the series or the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Time Series Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an exemplary time series dataset and first look at some initial rows and data types of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/AirPassengers.csv')\n",
    "print(data.head(), '\\n Data Types: \\n',data.dtypes )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains a particular month and number of passengers travelling in that month. But this is still not read as a TS object as the data types are ‘object’ and ‘int’. In order to read the data as a time series, we have to pass special arguments to the read_csv command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/AirPassengers.csv', index_col='Month', parse_dates=['Month'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s understand the arguments one by one:\n",
    "\n",
    "**parse_dates:** This specifies the column which contains the date-time information. As we say above, the column name is ‘Month’.\n",
    "\n",
    "**index_col:** A key idea behind using Pandas for TS data is that the index has to be the variable depicting date-time information. So this argument tells pandas to use the ‘Month’ column as index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TS is said to be stationary if its statistical properties such as mean and variance remain constant over time. But why is this important? Most of the TS models work on the assumption that the TS is stationary. Intuitively, we can set that if a TS has a particular behaviour over time, there is a very high probability that it will follow the same in the future. Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series.\n",
    "\n",
    "Stationarity is defined using very strict criteria. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n",
    "\n",
    "- constant mean\n",
    "- constant variance\n",
    "- an autocovariance that does not depend on time.\n",
    "\n",
    "First, simply plot the data and analyze it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly evident that there is an overall increasing trend in the data along with some seasonal variations. However, it might not always be possible to make such visual inferences. So, more formally, we can check for stationarity using the following:\n",
    "\n",
    "- **Plotting Rolling Statistics:** We can plot the moving average or moving variance and see if it varies with time. By moving average/variance we mean that at any instant `t`, we’ll take the average/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.\n",
    "\n",
    "- **Dickey-Fuller Test:** This is one of the statistical tests for checking stationarity. Here, the null hypothesis is that the TS is non-stationary. The test results consist of a `Test Statistic` and some `Critical Values` for different confidence levels. If the `Test Statistic` is less than the `Critical Value`, we can reject the null hypothesis and say that the series is stationary.\n",
    "\n",
    "These concepts might not sound very intuitive at this point. If you are interested in some theoretical background, you can refer to `Introduction to Time Series and Forecasting` by Brockwel and Davis. The book is a bit stats-heavy, but if you have the skill to read-between-lines, you can understand the concepts and tangentially touch the statistics.\n",
    "\n",
    "Back to checking for stationarity, we’ll be using the rolling statistics plots along with the Dickey-Fuller test results. Below, we define a function which takes a TS as input and generates both for us. Please note that we’ve plotted standard deviation instead of variance to keep the unit similar to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    # determine rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12).mean()\n",
    "    rolstd = timeseries.rolling(window=12).std()\n",
    "\n",
    "    # plot rolling statistics\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:\\n')\n",
    "    dftest = adfuller(timeseries)\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for stationarity for our time series dataset\n",
    "test_stationarity(data['#Passengers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the variation in standard deviation is rather small, the mean is clearly increasing with time - this is not a stationary series. Also, the test statistic is way higher than the critical values - we clearly fail to reject the null hypothesis of non-stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Time Series Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though stationarity is assumed in many TS models, almost no practical time series data are stationary. So statisticians have figured out ways to make series stationary, which we’ll discuss now. Actually, it is almost impossible to make a series perfectly stationary, but we try to take it as close as possible.\n",
    "\n",
    "Lets first understand what is making a TS non-stationary. There are 2 major reasons:\n",
    "1. **Trend** – varying mean over time. In our example dataset we saw that on average, the number of passengers was growing over time.\n",
    "2. **Seasonality** – variations at specific time-frames. E.g. people might have a tendency to buy cars in a particular month because of bonus payments.\n",
    "\n",
    "The underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then, statistical forecasting techniques can be implemented on this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.\n",
    "\n",
    "Note: We’ll be discussing a number of methods. Some might work well in this case and others might not. But the idea is to get a hang of all the methods and not focus on just the problem at hand.\n",
    "\n",
    "Let’s start by working on the trend part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating & Eliminating Trend\n",
    "\n",
    "One of the first tricks to reduce trend can be **transformation**. For example, in this case we can clearly see that there is a significant positive trend. So, we could apply transformation which penalize higher values more than smaller values. These include taking a `log, square root, cube root`, etc. Let's try a log-transform here for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement and evaluate log-transformation\n",
    "ts_log = np.log(data['#Passengers'])\n",
    "test_stationarity(ts_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use some techniques to estimate or model the trend and then remove it from the series. There can be many ways of doing it and some of the most commonly used ones are:\n",
    "\n",
    "- Aggregation – taking averages for a time period like monthly/weekly averages\n",
    "- Smoothing – taking rolling averages\n",
    "- Polynomial Fitting – fit a regression model\n",
    "\n",
    "We will discuss smoothing here, but you should try other techniques as well which might work out better for other problems. Smoothing refers to taking rolling estimates, i.e. considering the past few instances. There can be various ways but we will discuss two of those here.\n",
    "\n",
    "**Moving average**: In this approach, we take an average of `k` consecutive values depending on the frequency of the time series. Here we can take the average over the past 1 year, i.e. the past 12 values. Pandas has specific functions defined for determining rolling statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement moving average\n",
    "moving_avg = ts_log.rolling(12).mean()\n",
    "plt.plot(ts_log)\n",
    "plt.plot(moving_avg, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line shows the rolling average. Lets subtract this from the original series. Note that since we are taking an average of the last 12 values, the rolling mean is not defined for the first 11 values. This can be observed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff = ts_log - moving_avg\n",
    "ts_log_moving_avg_diff.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the first 11 are NaN. Lets drop these NaN values and check the plots to check for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for stationarity\n",
    "ts_log_moving_avg_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a much better series. The rolling values appear to be varying slightly but there is no specific trend. Also, the test statistic is smaller than the 5% critical values, so we can say with 95% confidence that this is a stationary series.\n",
    "\n",
    "However, a drawback in this particular approach is that the time-period has to be strictly defined. In this case we can take yearly averages but in complex situations like forecasting a stock price, its difficult to come up with a number. So we take a ‘weighted moving average’ where more recent values are given a higher weight. There can be many technique for assigning weights. A popular one is **exponentially weighted moving average** where weights are assigned to all the previous values with a decay factor. This can be implemented in Pandas as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expwighted_avg=ts_log.ewm(halflife = 12).mean() # provides exponential weighted functions\n",
    "plt.plot(ts_log)\n",
    "plt.plot(expwighted_avg, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here the parameter ‘halflife’ is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain. Other parameters like span and center of mass can also be used to define decay (check out the documentation for details). Now, let’s remove this from the series and check for stationarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_ewma_diff = ts_log - expwighted_avg\n",
    "test_stationarity(ts_log_ewma_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TS has even lesser variations in mean and standard deviation in magnitude. Also, the test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it’ll work even with no previous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating & Eliminating Trend & Seasonality\n",
    "\n",
    "The simple trend reduction techniques discussed before don’t work in all cases, particularly the ones with high seasonality. Let's discuss two ways of removing trend and seasonality:\n",
    "\n",
    "1. Differencing – taking the differece with a particular time lag\n",
    "2. Decomposition – modeling both trend and seasonality and removing them from the model.\n",
    "\n",
    "**Differencing**\n",
    "\n",
    "One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well to improve stationarity. First order differencing can be done in Pandas as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log.shift(periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff = ts_log - ts_log.shift()\n",
    "plt.plot(ts_log_diff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to have reduced trend considerably. Lets verify using our plots and test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean and std. deviation have small variations with time. Also, the Dickey-Fuller test statistic is less than the 10% critical value, thus the TS is stationary with 90% confidence. We can also take second or third order differences, which might get even better results in certain applications. I leave it to you to try them out.\n",
    "\n",
    "**Decomposing**\n",
    "\n",
    "In this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned. Let's skip the statistics and come to the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(ts_log) # seasonal decomposition using moving averages\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_log, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the trend & seasonality are separated out from the data and we can model the residuals. Let's check stationarity of the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_decompose = residual\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "test_stationarity(ts_log_decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dickey-Fuller test statistic is significantly lower than the 1% critical value. So this TS is very close to stationary. You can try advanced decomposition techniques as well, which can generate better results. However, you should note that converting the residuals into original values for future data is not very intuitive in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting a Time Series\n",
    "\n",
    "We saw different techniques and all of them worked reasonably well for making the TS stationary. Let's construct a model on the TS after differencing as it is a very popular technique. Also, it's relatively easy to add trend and seasonality back into predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:\n",
    "\n",
    "- A strictly stationary series with no dependence among the values. This is the easy case where we can model the residuals as white noise. However, this is very rare.\n",
    "- A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.\n",
    "\n",
    "Let me give you a **brief introduction to ARIMA**. ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n",
    "\n",
    "- Number of AR (Auto-Regressive) terms (p): AR terms are just lags of the dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "\n",
    "- Number of Differences (d): These are the number of nonseasonal differences. In our case, we took the first order difference. So either we can pass our already differenced series and set d=0 or pass the original series and set d=1. Both will generate the same results.\n",
    "\n",
    "- Number of MA (Moving Average) terms (q): MA terms are lagged forecast errors in the prediction equation. For instance, if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at the ith instant and the actual value.\n",
    "\n",
    "\n",
    "An important concern here is how to determine the values of ‘p’ and ‘q’. We use two plots to determine these numbers. Let's discuss them first.\n",
    "\n",
    "1. **Autocorrelation Function (ACF):** This is a measure of the correlation between the TS with a lagged version of itself. For instance at lag 5, ACF would compare the series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n",
    "\n",
    "2. **Partial Autocorrelation Function (PACF):** This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. E.g. at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\n",
    "\n",
    "The ACF and PACF plots for the TS after differencing can be plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "lag_acf = acf(ts_log_diff, nlags=20)\n",
    "lag_pacf = pacf(ts_log_diff, nlags=20, method='ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ACF: \n",
    "plt.subplot(121) \n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "# plot PACF:\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the two dotted lines on either sides of 0 are the confidence interevals. The following rules of thumb apply:\n",
    "- If ACF cuts off sharply after lag q, then MA(q) model may be appropriate\n",
    "- If PACF cuts off sharply after lag p, then AR(p) model may be appropriate\n",
    "- If they decay gradually, a mixed model may be appropriate\n",
    "\n",
    "Based on the above plots, let's construct 3 different ARIMA models considering individual as well as combined effects. We will also print the residual sum of squares (RSS) for each.\n",
    "\n",
    "We need to load the ARIMA model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p,d,q values can be specified using the order argument of ARIMA, which takes a tuple (p,d,q). Let's model the 3 cases:\n",
    "\n",
    "**AR Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log_decompose, order=(2, 1, 0), freq='MS') # MS: month start  \n",
    "results_AR = model.fit()  \n",
    "plt.plot(ts_log_decompose)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_decompose)**2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MA Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log_decompose, order=(0, 1, 2), freq='MS')  \n",
    "results_MA = model.fit()  \n",
    "plt.plot(ts_log_decompose)\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_decompose)**2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combined Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_log_decompose, order=(2, 1, 2), freq='MS')  \n",
    "results_ARIMA = model.fit()  \n",
    "plt.plot(ts_log_decompose)\n",
    "plt.plot(results_ARIMA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_decompose)**2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the combined model performs best. Now, we are left with 1 last step, i.e. taking these values back to the original scale.\n",
    "\n",
    "**Taking it back to original scale**\n",
    "\n",
    "Since the combined model gave the best result, let's scale it back to the original values and see how well it performs there. The first step is to store the predicted results as a separate series and observe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n",
    "predictions_ARIMA_diff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these start from ‘1949-07-01’ and not the first month. Why? This is because we decomposed (the trend component seems to register after six month) and the first element doesn’t have anything before it to subtract from. The way to convert the differencing to log scale is to add these differences consecutively. An easy way to do it is to first determine the cumulative sum at index. The cumulative sum can be found as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to add them back to the trend and seasonality component. For this lets create a series with all values as base number and add the differences to it. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(0, index=ts_log.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(decomposition.trend, fill_value=0)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(decomposition.seasonal, fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first element is the base number itself and from thereon the values cumulatively added. Last step is to take the exponent and compare with the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "plt.plot(data, label=\"data\")\n",
    "plt.plot(predictions_ARIMA, label=\"predictions\")\n",
    "plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-data['#Passengers'])**2)/len(data)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can clearly see the artifact of where the decomposed time series trend and seasonality component were not extrapolated.\n",
    "This can be circumvented by using more elaborate decomposition techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
