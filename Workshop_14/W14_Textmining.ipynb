{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 13` - Text Mining and Natural Language Processing\n",
    "\n",
    "\n",
    "This set of notes focuses on the basics of processing free text data. In addition to the more structured relational data and graphs we have discussed previously, free text makes up one of the most common types of \"widely available\" data: web pages, unstructured \"comment\" fields in many relational databases, and many other easily-obtained large sources of data naturally come in free text form.  The notable difference, of course, is that unlike the data types we have discussed before, free text lacks the \"easily extractable\" structure inherent in the previous types of data we have considered.\n",
    "\n",
    "This is not to say, of course, that free text data lacks structure. Just the opposite: by its very definition free text usually needs to have meaning to the people who are reading that data.  But the task of actually understanding this structure is a problem that is still beyond the scope of \"generic\" data science tools, because it often involves human-level intelligence (or at least bringing to bear an enormous amount of external context) to understand the meaning behind the text. This is also naturally a debatable point, and this particular perspective on free text data is one that may need to be reconsidered based upon advances in the upcoming years. \n",
    "\n",
    "**Content**\n",
    "1. Tokenization\n",
    "    1. Simple Tokenizer\n",
    "    2. Sentence Tokenizer\n",
    "    3. Word Tokenizer\n",
    "2. Feature Reduction\n",
    "    1. Stop-Words\n",
    "    2. Stemming\n",
    "    3. Lemmatization\n",
    "3. Text Classification\n",
    "    1. Total Count\n",
    "    2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free text in data science\n",
    "\n",
    "As mentioned above, the goal of using free text in data science that we will consider here is to extract some meaningful information from the text, _without_ any deep understanding of its meaning. The reason for this is simple: extracting deep understanding from text is hard.\n",
    "\n",
    "\n",
    "### Natural language processing/understanding\n",
    "The general field of Natural Language Processing (some might differentiate between NLP and \"natural language understanding\", but for our purposes here you can think of these synonymously) looks to truly understand the structure behind free text, e.g. perform tasks like parse the sentences grammatically, describe the general entities and properties that the text is referring to, etc. But it is easy to come up with examples of why this can be hard. For example, consider the following case, known as a Winograd schema:\n",
    "\n",
    "> The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n",
    "\n",
    "If you read the sentence:\n",
    " \n",
    "> The city councilmen refused the demonstrators a permit because they feared violence.\n",
    "\n",
    "then it is fairly obvious that the \"they\" here refers to the city councilmen; fearing violence would not be a reason for denying someone a permit, so clearly it is the councilmen that fear violence. On the other hand, in the sentence\n",
    "\n",
    "> The city councilmen refused the demonstrators a permit because they advocated violence.\n",
    "\n",
    "the \"they\" term clearly applies to the demonstrators (the city councilmen presumably would not advocate for violence, let alone deny someone a permit because they, the councilmen, were advocating violence).  The point of all this (and this was the original point of these examples, which were originally proposed by Terry Winograd in 1972) is that when we do something \"simple\", like parse a sentence, we bring to bear an enormous amount of outside knowledge and context to this action. Unlike, say, XML documents, there is no internally-specified format that makes language unambiguous; it could be unambiguous only because of external context and knowledge, or it could even be completely ambiguous.  Or, in a different vein, there can be grammatically-incorrect sentences that still have clear syntactic meaning, and there can be grammatically-correct sentences that are meaningful.\n",
    "\n",
    "\n",
    "### Free text in data science\n",
    "\n",
    "Fortunately, for many data science tasks, we can still extract considerable information from text _while only understanding it at an extremely rudimentary level_. Consider the following two reviews for the same movie:\n",
    "\n",
    "> ... truly, a stunning exercise in large-scale filmmaking; a beautifully-assembled picture in which Abrams combines a magnificent cast with a marvelous flair for big-screen, sci-fi storytelling.\n",
    "\n",
    "and\n",
    "\n",
    "> It's loud and full of vim -- but a little hollow and heartless.\n",
    "\n",
    "Understanding that the first review is positive, while the second review is negative, doesn't take any deep understand of the language itself, it can be done by simple keyword lookup: \"stunning\" and \"marvelous\" are associated with a positive review, while \"hollow\" and \"heartless\" are associated with negative reviews. Now, of course, it's possible to use more complex language to signify a positive review while using some \"negative\" words, with statements like, \"not at all boring\". But people don't usually write exclusively in this manner (doing so would be \"not at all clear\"), so that the general sentiment of text can still come through very easily even with a few instances where the words themselves can throw you off.\n",
    "\n",
    "\n",
    "**Terminology:** Before we begin, a quick note on terminology. In these notes, \"document\" will mean an individual group of free text data (this could be an actual document or a text field in a database). \"Words\" or \"terms\" refer to individual tokens separated by whitespaces, and additionally also refers to punctuation (so we will often separate punctuation explicitly from the surrounding words). \"Corpus\" refers to a collection of documents, and we will sometimes refer to the set of all _unique_ words/tokens in the corpus as the \"vocabulary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods of text mining learned in the lecture are presented in this tutorial. We will deal extensively with tokenization and feature reduction methods. Then we will implement a simple text classification algorithm. We will herein examine the Bag of Word model in more detail and will implement both \"Total Count\" and \"TF-IDF\".\n",
    "\n",
    "For this topic we will use the Natural Language Toolkit, `nltk` library: https://www.nltk.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Bag of words models and TF-IDF\n",
    "\n",
    "The bag of words model is by far the most common means of representing documents in data science. Under this model, a document is described soley by the set of words (and possibly their counts) that make up the document. All information about the actual ordering of the words is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the first time that you use `nltk`, you need to install the english text corpus and grammar once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(['punkt', 'punkt_tab', 'stopwords', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Hello Mr. Ritter, how are you doing today? The University of Cologne is awesome. \n",
    "The exam will be quite easy. You shouldn't be worried. Are you interested in studying here?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re` is a useful package in python for text processing services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trivial approach to tokenize text with regular expressions\n",
    "import re \n",
    "tokenized_text=re.split(\"\\.|\\?\", text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence tokenization using nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenized_text = sent_tokenize(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization using nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze frequency distribution of words\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "In the lecture we learned that we use a dedicated feature dimension (i.e., column) for each word. However, most languages consist of several hundred thousand up to millions of words. Accordingly, the feature space becomes very large and we quickly reach our limits (e.g., Curse of Dimensionality). Text Mining offers us methods to reduce the number of dimensions. In the following we will look at \"Stop-Word\", \"Stemming\" and \"Lemmatization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter out stop words\n",
    "filtered_sent=[]\n",
    "for w in tokenized_word: # tokenized words from my input text\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w) # use this word in machine learning tasks\n",
    "print(\"Tokenized Sentence:\\n \",tokenized_word)\n",
    "print(\"\\n Filterd Sentence:\\n \",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stemming - reduce a word into its word root, that meaning, cut off the suffixes. \n",
    "# note that this could lead to stems which are not grammatically correct.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer() # https://de.wikipedia.org/wiki/Porter-Stemmer-Algorithmus\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\\n \",filtered_sent)\n",
    "print(\"Stemmed Sentence:\\n \",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization - reduce a word to its base word. \n",
    "# this method is more sophisticated than stemming since it leverage contextual information and a dictionary.\n",
    "# however, results may depend on the chosen dictionary.\n",
    "\n",
    "#nltk.download('wordnet') # WordNet is just another NLTK corpus reader, and can be imported like this:\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words=[]\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w,\"v\"))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Lemmatized Sentence:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the obvious information we are throwing away with this representation, it send to work surpringingly well in practice, for the precise reason we mentioned above, that the general \"gist\" of many documents can be obtained by only looking at the presence/absence of words in the text.\n",
    "\n",
    "In these notes, we'll cover a simple example of creating so-called TF-IDF vectors, which represent the documents via a (weighted) bag of words model. We will then using these to compute the similarity between documents.  This technique is a common approach for applications like document retrieval and/or search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency\n",
    "\n",
    "In our example, let's begin with a corpus that contains the following three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"the goal of this lecture is to explain the basics of free text processing\",\n",
    "             \"the bag of words model is one such approach\",\n",
    "             \"text processing via bag of words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting, we can represent the documents using a _term frequency_ matrix, an $m \\times n$ matrix where $m$ denotes the number of documents, and $n$ denotes the vocabulary size (i.e., the number of unique words across all documents). To see (the naive way of) how to construct this list, let's first consider a simple way to get a list of all unique words across all documents. In general there is no need to actually sort the list of words, but we will do so for simplicity here. It's a good idea to also generate a dictionary that maps words to their index in this list, as we'll frequently want to look up the index corresponding to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_words = [doc.split() for doc in documents]\n",
    "vocab = sorted(set(sum(document_words, [])))\n",
    "vocab_dict = {k:i for i,k in enumerate(vocab)}\n",
    "print(vocab, \"\\n\")\n",
    "print(vocab_dict, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct a matrix that contains word counts (term frequencies) for all the documents.  We'll also refer to the term frequency of the $j$ th word in the $i$ th library as $\\mathrm{tf}_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_tf = np.zeros((len(documents), len(vocab)), dtype=int)\n",
    "for i,doc in enumerate(document_words):\n",
    "    for word in doc:\n",
    "        X_tf[i, vocab_dict[word]] += 1\n",
    "print(X_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, each of the _rows_ in this matrix correponds to one of the three documents above, and each column correponds to one of the 19 words.  Importantly, note that in practice, you will want to create term frequency matrices directly in sparse format, because the term frequency matrix is itself typically sparse (when there are a large number of documents, many words will only be contained in a small number of the document).\n",
    "\n",
    "In this case, we had the entry in our term frequency matrix just correspond to the number of occurences of that term.  But there are other possibilities as well:\n",
    "\n",
    "* The entry can be binary: 1 if the term occurs (any number of times), and 0 otherwise.  This somewhat mitigates the significance of common words that may occur very frequently.\n",
    "* A nonlinear scaling, e.g. $\\log (1+\\mathrm{tf}_{i,j})$, which lies somewhere between the binary case and the raw counts.\n",
    "* A scaled version of term frequencies, e.g., scaling by the maximum term frequency in the document, $\\mathrm{tf}_{i,j} / \\max_k \\mathrm{tf}_{i,k}$.  (Note that this won't affect the actual similarity scores we'll discuss next, because we will ultimately scale each document but it affects the term frequency matrix itself, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse document frequency\n",
    "\n",
    "An obvious issue with using normal term frequency counts to represent a document is that the document's vector (and the resulting similarities we will consider) will often be \"dominated\" by very common words, for example: \"of\", \"the\", \"is\", in the preceeding example documents.  This issue can be mitigated to some extent by excluding so-called \"stop words\" (common English words like \"the\", \"a\", \"of\" that aren't considered relevant to the particular documents) from the term frequency matrix. But this still ignores the case where a word that may not be a generic stop word still happens to occur in a very large number of documents. Intuitively, we expect that the most \"important\" words in a document are precisely those that only occur in some relatively small number of documents, so that we want to discount the weight of very frequently-occurring terms.\n",
    "\n",
    "This can be accomplished via the inverse document frequency weight for words. Just as with term frequencies, there are some different weightings of this term, but the most common formulation is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{idf}_j = \\log\\left(\\frac{\\# documents}{\\# documents with word j}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "As an example, if the word is contained in every document, then the inverse document frequency weight will be zero (log of one).  In contrast, if a word occurs in only one document, its inverse document frequency will be $\\log (\\# documents)$.\n",
    "\n",
    "Note that inverse document frequency is a _per word_ term, as opposed to term frequency, which is _per word and document_.  We can compute inverse document frequency for our data set as follows, which mainly just requires counting how many documents contain each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(X_tf.shape[0]/X_tf.astype(bool).sum(axis=0))\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "The term frequency inverse document frequency (TF-IDF) combination simply scales the columns of the term frequency matrix by their inverse document frequency. In doing so, we still have an effective bag of words representation of each document, but we do so with the weighting implied by the inverse document frequency: discouting words that occur very frequently, and increasing the weight of less frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = X_tf * idf\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Given a TF-IDF (or just term frequency) matrix, one of the more common questions to address is to compute similarity between multiple documents in the corpus.  The common metric for doing so is to compute the cosine similarity between two different documents. This is simply a normalized inner product between the vectors describing each documents. Specifically,\n",
    "$$\n",
    "\\text{CosineSimilarity}(x, y) = \\frac{x^T y}{\\|x\\|_2 \\cdot \\|y\\|_2}\n",
    "$$\n",
    "\n",
    "The cosine similarity is a number between zero (meaning the two documents share no terms in common) and one (meaning the two documents have the exact same term frequency or TF-IDF representation). In fact, the cosine similarity is exactly the complement of the squared Euclidean distance between the normalized document vectors; formally, for $\\tilde{x} = x / \\|x\\|_2$ and $\\tilde{y} = y / \\|y\\|_2$, \n",
    "$$\n",
    "\\frac{1}{2}\\|\\tilde{x} - \\tilde{y}\\|_2^2 \n",
    "= \\frac{1}{2}(\\tilde{x} - \\tilde{y})^T (\\tilde{x} - \\tilde{y}) \\\\\n",
    "= \\frac{1}{2} (\\tilde{x}^T \\tilde{x} - 2 \\tilde{x}^T \\tilde{y} + \\tilde{y}^T \\tilde{y}) \\\\\n",
    "= \\frac{1}{2} (1 - 2 \\tilde{x}^T \\tilde{y} + 1) \\\\\n",
    "= 1 - \\text{CosineSimilarity}(x,y)\n",
    "$$\n",
    "\n",
    "We can compute cosine similarity between the TF-IDF vectors in our corpus as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_norm = X_tfidf / np.linalg.norm(X_tfidf, axis=1)[:,None]\n",
    "M = X_tfidf_norm @ X_tfidf_norm.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at the cosine similarity with the ordinary term frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_norm = X_tf / np.linalg.norm(X_tf, axis=1)[:,None]\n",
    "M = X_tf_norm @ X_tf_norm.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, using the term frequency matrix results in substantially higher scores: the inclusion of words that occur across many of the documents add positive terms to the inner product between the document vectors, resulting in higher similarity scores.  Note, however, that the distances are typically _all_ scaled up, meaning that the relative distances are not necssarily any more informative than the TF-IDF similarities (indeed, it is typically less informative, since the scores are inflated by the \"random\" occurances of multiple high-frequency words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the techniques demonstrated above — Bag of Words and TF-IDF vectorization — we can now represent text documents as numerical vectors. These representations can serve as input features for a variety of machine learning tasks. For instance, we could use them for text classification, where a model learns to predict categories such as spam vs. non-spam emails or sentiment (positive/negative) of product reviews. Alternatively, these vectors could be used in unsupervised clustering, allowing us to automatically group similar documents together, for example, to discover topics in a large news corpus. Finally, the cosine similarity measure we computed can be applied in information retrieval or recommendation systems, helping to find the most similar documents to a query or to identify near-duplicate content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
