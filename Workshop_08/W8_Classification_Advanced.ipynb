{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AA Workshop 08` - Probabilistic Classification & Non-Linear Features\n",
    "\n",
    "In this workshop we will continue our discussion of classification. Last week we introduced basic linear classification and evaluation of classifiers, this week we will dive into probabilistic classification and using non-linear features.\n",
    "\n",
    "We will cover the following: \n",
    "1. Logistic Regression\n",
    "1. Naive Bayes\n",
    "1. Non-Linear Features\n",
    "1. Exemplary Model Development Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce classification problems, we're going to continue with the example of identifying whether cancer cells from a biopsy are malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True) # suppress scientific notation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "print(\"Number of benign samples:\", len(cancer_df[cancer_df[\"diagnosis\"]==\"B\"]))\n",
    "print(\"Number of malignant samples:\", len(cancer_df[cancer_df[\"diagnosis\"]==\"M\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression results from choosing the logistic loss\n",
    "\\begin{equation}\n",
    "\\ell_{\\mathrm{logistic}}(h_\\theta(x), y) = \\log(1+\\exp(-h_\\theta(x) \\cdot y))\n",
    "\\end{equation}\n",
    "as our classification loss to minimize.  Logistic regression also has a nice probabilistic interpretation: certain quantities give the _probability_, under a particular model, of an example being positive or negative. We will consider this probabilistic setting later, but for now we are going to simply treat it as another loss minimization algorithm.\n",
    "\n",
    "Assuming a linear hypothesis function (the typical assumption under logistic regression), the partial deriative of the logistic loss is given by\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta_j} \\log(1+\\exp(-\\theta^T x \\cdot y)) & = \\frac{1}{1+\\exp(-\\theta^T x \\cdot y)} \\frac{\\partial}{\\partial \\theta_j} (1+\\exp(-\\theta^T x \\cdot y))\\\\  \n",
    "& = - \\frac{\\exp(-\\theta^T x \\cdot y)}{1+\\exp(-\\theta^T x \\cdot y)} x_j\\cdot y \\\\\n",
    "& = -\\frac{1}{1+\\exp(\\theta^T x \\cdot y)} x_j\\cdot y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where in the last line we use the fact that\n",
    "\\begin{equation}\n",
    "\\frac{\\exp(-x)}{1+\\exp(-x)} = \\frac{\\exp(-x)}{1+\\exp(-x)} \\cdot \\frac{\\exp(x)}{\\exp(x)} = \\frac{1}{1+\\exp(x)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus, the gradient of the entire objective function for logistic regression (we'll omit any regularization term, though we could also add this if desired) is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla_\\theta E(\\theta) & = \n",
    "\\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^m \\log \\left (1+ \\exp \\left (\\theta^T x^{(i)} \\cdot y^{(i)} \\right )\\right) \\\\\n",
    "& = \\frac{1}{m} \\sum_{i=1}^m -x^{(i)} y^{(i)} \\frac{1}{1+\\exp\\left(\\theta^T x^{(i)} \\cdot y^{(i)}\\right)}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We can use this derivation to write the gradient descent procedure for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this exercise, however, we will use the readily available `LogisticRegression` class in scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use `area_mean` as the single predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: make sure to standardize your input features using the `StandarScaler` class in scikit learn. This should be the default approach in any classification setting! The `StandarScaler` has a similar interface as the machine learning classes, where you first initialize, then use `.fit()` (which computes the mean and variance of a column), and `.transform()` (which converts the data to have zero mean and unit variance). `StandardScaler` standardizes to 0 mean and unit variance. Also note that we do not actually have to re-scale our target variable; LogisticRegression can handle the labels as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y\n",
    "x = cancer_df['area_mean'].values.reshape(-1,1)\n",
    "y = cancer_df['diagnosis']\n",
    "\n",
    "# standardize x\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "norm = StandardScaler() # initialize\n",
    "X_norm = norm.fit_transform(x)  # simultaneously fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions for plotting\n",
    "def rescale_classes(class_membership, true_label='M'):\n",
    "    return (class_membership == true_label).astype(int) * 2 - 1\n",
    "\n",
    "def plot_cells(variable='area_mean'):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    malign = cancer_df[cancer_df[\"diagnosis\"]==\"M\"]\n",
    "    benign = cancer_df[cancer_df[\"diagnosis\"]==\"B\"]\n",
    "    plt.scatter(benign[variable], rescale_classes(benign['diagnosis']), marker='x', color='orange')\n",
    "    plt.scatter(malign[variable], rescale_classes(malign['diagnosis']), marker='x', color='red')\n",
    "    plt.plot([0, 2600], [0,0], color='lightgrey', linestyle='dotted')\n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([-1.1,1.1])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit model\n",
    "model_log_one = LogisticRegression(C=100)   # C is a regularization term, we set it to 100 here\n",
    "model_log_one.fit(X_norm, y)\n",
    "\n",
    "# plot\n",
    "plot_cells()\n",
    "xfit = np.linspace(0, 2600)\n",
    "xfit_scaled = norm.transform(xfit.reshape(-1,1))\n",
    "plt.plot([680, 680], [-1.1, 1.1], color='grey', linestyle='--')\n",
    "plt.plot(xfit, model_log_one.decision_function(xfit_scaled.reshape(-1,1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the graph above, we plotted the raw outputs from our hypothesis function using the `decision_function`-method. The `predict`-method, on the other hand, gives us the predicted class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 outputs of the decision_function-method\n",
    "model_log_one.decision_function(X_norm)[:10].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print corresponding predictions using predict-method\n",
    "predictions = model_log_one.predict(X_norm)\n",
    "predictions[:10].reshape(-1,1) # print first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice properties of a logistic regression is the possibility to return not only the raw predictions (from the hypothesis function) or the predicted class labels (using the predict-method), but we can also return the predicted probabilities of belonging to a class using the `predict_proba`-method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Order of classes:\", model_log_one.classes_)\n",
    "model_log_one.predict_proba(X_norm)[:10] # return predicted probabilities for first 10 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to understand how the outputs from `decision_function`, `predict` and `predict_proba` interrelate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background on probabilities:** how does a logistic regression generate these probabilities? The key is a simple transformation of our linear hypothesis function. Remember that our hypothesis function $h_\\theta(x)$ is just a linear combination of our input features and their respective coefficients. The resulting values are represented by the blue line in the plot above. The output from our linear hypothesis function is unbounded, meaning that predicted values can theoretically vary from very positive to very negative numbers. Since we are predicting only two classes, this is not ideal. Instead, what we really want are predictions that are restricted to a range of 0 and 1 - which would enable interpreting the predictions as probabilities. The sigmoid transformation, defined as $1/(1+e^{-h_\\theta(x)})$ does exactly that. We can visualize this using our plot above, but changing the scale of our y-axis to 0 and 1 instead of -1 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "malign = cancer_df[cancer_df[\"diagnosis\"]==\"M\"]\n",
    "benign = cancer_df[cancer_df[\"diagnosis\"]==\"B\"]\n",
    "plt.scatter(benign['area_mean'], benign['diagnosis'], marker='x', color='orange')\n",
    "plt.scatter(malign['area_mean'], malign['diagnosis'], marker='x', color='red')\n",
    "plt.plot([0, 2600], [0.5,0.5], color='lightgrey', linestyle='dotted')\n",
    "plt.xlim([0,2600])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.xlabel(\"Mean Area\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.yticks(ticks = [0,1], labels=[0,1])\n",
    "xfit = np.linspace(0, 2600)\n",
    "xfit_scaled = norm.transform(xfit.reshape(-1,1))\n",
    "plt.plot([680, 680], [-1.1, 1.1], color='grey', linestyle='--')\n",
    "plt.plot(xfit, model_log_one.predict_proba(xfit_scaled.reshape(-1,1))[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the blue curve illustrates the probability of belonging to class \"malignant\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include a second predictor, namely `concave points_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting\n",
    "def plot_classification_sklearn(model, norm):\n",
    "    x_range = [0,2600]\n",
    "    y_range = [0, 0.21]\n",
    "    plt.figure(figsize = (10,8))\n",
    "    \n",
    "    # plot classifcation regions\n",
    "    grid=200\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),  # create coordinate mesh\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "    \n",
    "    X_ = np.array([np.ravel(xx), np.ravel(yy)]).T\n",
    "    zz = model.decision_function(norm.transform(X_)).reshape(grid,grid)\n",
    "    cs = plt.contourf(xx,yy,zz, zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C0')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C3')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Mean Concave Points\")\n",
    "    plt.legend([s1,s2],['Malignant', 'Benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "norm = StandardScaler()\n",
    "X_norm = norm.fit_transform(X)  # simultaneously fit and transform \n",
    "\n",
    "model_log_two = LogisticRegression(C=1e2)   # C is a regularization term, we set it to 100 here\n",
    "model_log_two.fit(X_norm, Y)\n",
    "\n",
    "plot_classification_sklearn(model_log_two, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model perform (on the training dataset)? Let us look at the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((model_log_two.predict(X_norm) == Y).mean())  # Note: mean() operation on booleans returns the proportion of True\n",
    "\n",
    "# alternatively:\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(Y, model_log_two.predict(X_norm))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we used just two features to classify cancer cells. We achieved a decent accuracy of >90%. Can we impove performance even further by including more information, i.e. more features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect available features\n",
    "cancer_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = np.array(cancer_df[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst']])\n",
    "\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# transform\n",
    "norm_full = StandardScaler()\n",
    "X_norm_full = norm_full.fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train new model\n",
    "model_log_full = LogisticRegression(C=1e2)\n",
    "model_log_full.fit(X_norm_full,Y)\n",
    "\n",
    "# return acuracy\n",
    "print((model_log_full.predict(X_norm_full) == Y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a machine learning algorithm that rests heavily on probabilistic modeling, particularly Bayesâ€™ Theorem. In contrast to Bayes' Theorem, we make the \"naive\" assumption that all features are conditionally independent given the class. As a result, we estimate class probabilities as\n",
    "\n",
    "\\begin{equation}\n",
    "p(y \\mid X) = \\frac{p(X \\mid y) \\, p(y)}{p(X)} = \\frac{\\prod_i p(x_i \\mid y) p(y)}{\\prod_i p(x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "To estimate $p(x_i \\mid y)$, Naive Bayes makes assumptions about the underlying distribution for each $x_i$ within each class $y$. The most commonly assumed distribution for numerical features is the Gaussian distribution, which is implemented in the Gaussian Naive Bayes algorithm `GaussianNB` in sklearn. We will illustrate this using our example with the two numerical features `area_mean` and `concave points_mean`.\n",
    "\n",
    "Note that since Naive Bayes looks at the distribution of each feature separately within each class, it is typically not required to standardize features beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the plotted histograms of our two features by class, the Gaussian assumption seems reasonable in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=cancer_df, x='area_mean', hue=\"diagnosis\", kde=True, bins=30, alpha=0.5)\n",
    "plt.title(f\"Histogram of area_mean\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=cancer_df, x='concave points_mean', hue=\"diagnosis\", kde=True, bins=30, alpha=0.5)\n",
    "plt.title(f\"Histogram of concave points_mean\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# train model\n",
    "model_gnb = GaussianNB()\n",
    "model_gnb.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance (on training data)\n",
    "print((model_gnb.predict(X) == Y).mean())  # Note: mean() operation on booleans returns the proportion of True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: keep in mind that `GaussianNB` assumes a Gaussian distribution and is, therefore, only suitable for numerical features. If you have categorical predictors, you need to use models that assume different underlying distributions (such as `BernoulliNB` or `CategoricalNB`). Please refer to the [sklearn documentation](scikit-learn.org/stable/modules/naive_bayes.html) for all available alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Non-Linear Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that when discussing linear regression a couple of weeks ago, we tried to predict peak electricity demand using peak temperature. Eventually, we did not only include peak temperature, but instead generated additional features from this raw input using polynomials or RBFs. You guessed it, we can do exactly the same for classification. \n",
    "\n",
    "Just like for linear vs. non-linear regression, the application to the classification setting is just a matter of plugging in the relevant features into our classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by considering again our example of the breast cancer cell classification task, loading the data and running a linear SVM on the two-feature reduced example. Note how we already use the polynomial features method for that, using degree one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC # Linear Support Vector Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "X_norm = norm.fit_transform(X)  # simultaneously fit and transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feat_sklearn (X, deg):\n",
    "    model = PolynomialFeatures(deg)\n",
    "    X = model.fit_transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_sklearn_non_lin(model, norm, deg):\n",
    "    x_range = [0,2600]\n",
    "    y_range = [0, 0.21]\n",
    "    plt.figure(figsize = (10,8))\n",
    "    \n",
    "    # plot classifcation regions\n",
    "    grid=200\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),\n",
    "                        np.linspace(y_range[0], y_range[1], grid))  # create coordinate mesh\n",
    "    \n",
    "    X_ = np.array([np.ravel(xx), np.ravel(yy)]).T\n",
    "    zz = model.decision_function(poly_feat_sklearn(norm.transform(X_),deg)).reshape(grid,grid)\n",
    "    cs = plt.contourf(xx,yy,zz, zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Mean Concave Points\")\n",
    "    plt.legend([s1,s2],['Malignant','Benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg=1\n",
    "Phi = poly_feat_sklearn(X_norm, deg)\n",
    "model_SVM_poly = LinearSVC(loss='hinge',max_iter=100000)\n",
    "model_SVM_poly.fit(Phi, Y)\n",
    "plot_classification_sklearn_non_lin(model_SVM_poly, norm, deg)\n",
    "print(accuracy_score(Y,model_SVM_poly.predict(Phi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg=2\n",
    "Phi = poly_feat_sklearn(X_norm, deg)\n",
    "model_SVM_poly = LinearSVC(loss='hinge',max_iter=100000)\n",
    "model_SVM_poly.fit(Phi, Y)\n",
    "plot_classification_sklearn_non_lin(model_SVM_poly, norm, deg)\n",
    "print(accuracy_score(Y,model_SVM_poly.predict(Phi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg=4\n",
    "Phi = poly_feat_sklearn(X_norm, deg)\n",
    "model_SVM_poly = LinearSVC(loss='hinge',max_iter=100000)\n",
    "model_SVM_poly.fit(Phi, Y)\n",
    "plot_classification_sklearn_non_lin(model_SVM_poly, norm, deg)\n",
    "print(accuracy_score(Y,model_SVM_poly.predict(Phi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg=8\n",
    "Phi = poly_feat_sklearn(X_norm, deg)\n",
    "model_SVM_poly = LinearSVC(loss='hinge',max_iter=100000)\n",
    "model_SVM_poly.fit(Phi, Y)\n",
    "plot_classification_sklearn_non_lin(model_SVM_poly, norm, deg)\n",
    "print(accuracy_score(Y,model_SVM_poly.predict(Phi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, just like with the regression examples we saw, the model is probably starting to overfit.  And just like before, we can counteract this to some extent by increasing the regularization parameters (or in scikit learn's terminology, decreasing the $C$ parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg=8\n",
    "Phi = poly_feat_sklearn(X_norm, deg)\n",
    "model_SVM_poly = LinearSVC(loss='hinge',max_iter=100000, C=0.01)\n",
    "model_SVM_poly.fit(Phi, Y)\n",
    "plot_classification_sklearn_non_lin(model_SVM_poly, norm, deg)\n",
    "print(accuracy_score(Y,model_SVM_poly.predict(Phi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn actually contains a more general implementation of SVM classification (`SVC`; before we were using `LinearSVC`), which allows us to freely use other types of feature relationships, employing a so-called kernel (linear, RBF, poly, etc.). Let's have a look and let us consider some kernel-based functions. Note that in addition to adding the `degree=d` parameter, you'll want to set the `coef0=1.0` parameter to include the constant term `1` in the kernel function (without this, the polynomial kernel only has terms of degree _exactly_ $d$, and typically performs quite poorly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_sklearn(model, norm):\n",
    "    x_range = [0,2600]\n",
    "    y_range = [0, 0.21]\n",
    "    plt.figure(figsize = (10,8))\n",
    "    \n",
    "    # plot classifcation regions\n",
    "    grid=1000\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "    X_ = np.array([np.ravel(xx), np.ravel(yy)]).T\n",
    "    zz = model.decision_function(norm.transform(X_)).reshape(grid,grid)\n",
    "    cs = plt.contourf(xx,yy,zz, zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "    \n",
    "    # plot data points\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\")\n",
    "    plt.ylabel(\"Mean Concave Points\")\n",
    "    plt.legend([s1,s2],['Malignant','Benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "deg=4\n",
    "model = SVC(kernel='poly', C=100, degree=deg, coef0=1.0)\n",
    "model.fit(X_norm, Y)\n",
    "plot_classification_sklearn(model, norm)\n",
    "print(accuracy_score(Y,model.predict(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis function (RBF)\n",
    "\n",
    "If I were to make a completely anecdotal estimate, I would guess that the most frequently used type of nonlinear feature is not the polynomial, but something called the _radial basis function_, often abreviated as RBF.  Radial basis functions are similar to polynomials in that they are non-linear functions of the input data, but they are notably different in that they are generally _local_ features: the value of any particular feature is close to zero for most of the input space, but non-zero in a small region around a \"center\" parameter.  Let's start with the definition, and we can then provide some illustrations that hopefully make this more concrete.  To keep things simple at the start, we're only going to consider radial basis functions of one-dimensional raw inputs.  A radial basis function feature vector is defined as the following:\n",
    "$$\n",
    "\\phi : \\mathbb{R} \\rightarrow \\mathbf{R}^k = \\left[ \\begin{array}{c} \n",
    "\\exp \\left(\\frac{-(x - \\mu^{(1)})^2}{2\\sigma^2} \\right) \\\\\n",
    "\\exp \\left(\\frac{-(x - \\mu^{(2)})^2}{2\\sigma^2} \\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp \\left(\\frac{-(x - \\mu_{(k-1)})^2}{2\\sigma^2} \\right) \\\\\n",
    "1\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "where $\\mu^{(1)},\\ldots,\\mu^{(k-1)} \\in \\mathbb{R}$ (called the means) and $\\sigma \\in \\mathbb{R}$ (called the bandwidth) are the hyperparameters of this feature vector.  \n",
    "\n",
    "Let's look at a single one of these terms $\\phi_j(x)$ (this is the $j$th element of the feature vector, because remember $\\phi(x)$ outputs a $k$-dimensional vector).\n",
    "\\begin{equation}\n",
    "\\phi_j(x) = \\exp \\left(\\frac{-(x - \\mu^{(j)})^2}{2\\sigma^2} \\right)\n",
    "\\end{equation}\n",
    "If you're familiar with the Gaussian distribution, you may recognize this as looking similar to the density function of the Gaussian (though without the normalizing constant). One single dimension of this feature (for varying inputs $x$, and here assuming mean $\\mu^{(j)} = 1.5$ and $\\sigma = 0.4$) looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,4,100)\n",
    "mu_j = 1.5\n",
    "sigma = 0.5\n",
    "plt.plot(x,np.exp(-(x-mu_j)**2 / (2*sigma**2)))\n",
    "plt.plot([1.5,1.5], [0,1], 'k--')\n",
    "plt.text(1.55, 0, r\"$\\mu$\")\n",
    "plt.arrow(1.5, np.exp(-0.5), 0.5-0.1, 0, head_width=0.03, head_length=0.1, fc='k')\n",
    "plt.text(1.7, 0.55, r\"$\\sigma$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(r\"$\\phi_j(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature is largest (equal to one) when $x$ is equal to $\\mu$, but falls off very rapidly as $x$ moves away from $\\mu$ (proportional to the exponential of the negative squared difference, which will go effectively towards zero very quickly). Those who are familiar with Gaussian densities will notice the familiar formula and bell-shaped curve, where about two thirds of the area under the curve lies within $\\sigma$ of the center. Note, however, that despite superficial similarities, this is _not_ a Gaussian distribution: it is not scaled, and the maximum value for any specific feature is always 1.0 for $x = \\mu_j$. To emphasize this difference further, some definitions of RBFs define them just in terms of a different scaling parameter instead of the bandwidth\n",
    "\\begin{equation}\n",
    "\\phi_j(x) = \\exp(-\\gamma (x-\\mu^{(j)})^2)\n",
    "\\end{equation}\n",
    "which, of course, is equivalent to the above if we set $\\gamma = 1/(2\\sigma^2)$. However, it's a bit more common to see the bandwidth parameter formulation, just because it _does_ let you use the general intuition about the size of a bell curve that many people have from using Gaussian distributions.\n",
    "\n",
    "To get some intuition about how these RBFs work as features, consider a feature vector with $k=10$ (i.e., 9 RBF features plus a constant term), with $\\mu^{(j)} = 0,0.5,1.0,\\ldots,4.0$ and $\\sigma=0.5$.  The set of all RBFs looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.5,4.5,100)\n",
    "mu = np.linspace(0,4,9)\n",
    "sigma = 0.5\n",
    "for mu_ in mu:\n",
    "    plt.plot(x, np.exp(-(x-mu_)**2 / (2*sigma**2)))\n",
    "plt.plot([-0.5,4.5], [1,1])\n",
    "plt.xlim([-0.5,4.5])\n",
    "plt.legend([r\"$\\phi_{\" + str(j+1) + \"}(x)$\" for j in range(10)], bbox_to_anchor=(1.02,0.95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a two-dimensional space, this looks like a mountain-range (when you use hiking maps with contour lines) from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "sig = d/2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "XY = np.meshgrid(np.linspace(-1,1,d), np.linspace(-1,1,d))\n",
    "mu = np.array([XY[0].ravel(), XY[1].ravel()]).T\n",
    "\n",
    "XX,YY = np.meshgrid(np.linspace(-1.25,1.25,100), np.linspace(-1.25,1.25,100))\n",
    "# sig = 0.25\n",
    "for mu_ in [mu[0], mu[-1]]: # we only visualize the top-right and bottom-left for visibility\n",
    "    ZZ = np.exp(-((XX-mu_[0])**2 + (YY-mu_[1])**2)/(2*sig**2))\n",
    "    plt.contourf(XX,YY,ZZ, alpha=.1, cmap='Blues')\n",
    "plt.scatter(mu[:,0], mu[:,1])\n",
    "\n",
    "ax.set_ylabel('Scaled (-1,1) Mean # Concave Points')\n",
    "ax.set_xlabel('Scaled (-1,1) Mean Area (pixels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of non-linear fitting with RBFs is to approximate the underlying function with a linear combination of these features. By combining them in the proper manner, it is possible to approximate very general functions.\n",
    "\n",
    "Note also that there is no need to normalize the data, because the RBF features will always be scaled to be between zero and one (we could further normalize the generated features themselves, but this is typically not needed, as the features by definition will already be scaled to the range $[0,1]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 / (len(X_norm) * X_norm.var()) # scikit-learn uses gamma = 1/(n_features*X.var()) parameterization\n",
    "model_rbf = SVC(kernel='rbf', C=10.0, gamma=100*gamma)\n",
    "model_rbf.fit(X_norm, Y)\n",
    "plot_classification_sklearn(model_rbf, norm)\n",
    "print(accuracy_score(Y,model_rbf.predict(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, because all these example admittedly look fairly similar (the decision boundary _is_ roughly linear with some noise, after all), let's consider a case where we make the bandwidth small enough so that the method is guaranteed to overfit to the training data. Here we make the bandwidth very small (i.e., $\\gamma$ large), and the regularization small, so that the classifier actually manages to get almost 100% accuracy on the training data (of course with very poor generalization performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 / (len(X_norm) * X_norm.var()) # scikit-learn uses gamma = 1/(n_features*X.var()) parameterization\n",
    "model_rbf = SVC(kernel='rbf', C=10.0, gamma=100000*gamma)\n",
    "model_rbf.fit(X_norm, Y)\n",
    "plot_classification_sklearn(model_rbf, norm)\n",
    "print(accuracy_score(Y,model_rbf.predict(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing to keep in mind through all this is how the non-linear features and hyperparameter settings will influence the ability of functions to represent complex data and to overfit and underfit the data. And, of course, when using scikit-learn, you'll need to understand the documentation to the point where you see how the hyperparameters of the class of interest map to the parameters that you actually want to control. This usually requires studying the documentation quite a bit, but it's important, and the default parameters are often quite poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Implementation with Proper Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above implementations have been exclusively designed to illustrate the workings of relevant classification algorithms. One issue with that is that we have trained our algorithms on the full set of available data. The classification metrics are, therefore, relatively meaningless as we need to evaluate on previously unseen data. The following code showcases a more realistic model development routine to train a high-performing classification algorithm for the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper functions first. First, we want to know how the model performs, but sometimes without test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, norm=None, test = False):\n",
    "    if norm is not None:\n",
    "        X_train_norm = norm.fit_transform(X_train[:,0:2])\n",
    "        X_val_norm = norm.fit_transform(X_val[:,0:2])\n",
    "        X_test_norm = norm.fit_transform(X_test[:,0:2])\n",
    "    else:\n",
    "        X_train_norm = X_train[:,0:2]\n",
    "        X_val_norm = X_val[:,0:2]\n",
    "        X_test_norm = X_test[:,0:2]\n",
    "    \n",
    "    print(f'Training accuracy: {accuracy_score(y_train, model.predict(X_train_norm))}')\n",
    "    print(f'Validation accuracy: {accuracy_score(y_val, model.predict(X_val_norm))}')\n",
    "    print(f'Test accuracy: {accuracy_score(y_test, model.predict(X_test_norm))}') if test else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a helper function that wraps most of the usual steps and visually compares how the different sets perform on the fitted decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aio_helper(x1, x2, model, norm=None, test=False):\n",
    "    if norm is not None:\n",
    "        X_norm = norm.fit_transform(X_train[:,0:2])\n",
    "    else:\n",
    "        X_norm = X_train[:,0:2]\n",
    "    \n",
    "    model.fit(X_norm, y_train) # drop ID from features\n",
    "    \n",
    "    # look how we did:\n",
    "    score_model(model, norm=norm, test = test)\n",
    "\n",
    "    x_range = [cancer_df[x1].min()-(cancer_df[x1].mean()*0.1), cancer_df[x1].max()+(cancer_df[x1].mean()*0.1)]\n",
    "    y_range = [cancer_df[x2].min()-(cancer_df[x2].mean()*0.1), cancer_df[x2].max()+(cancer_df[x2].mean()*0.1)]\n",
    "\n",
    "    # set up plotting\n",
    "    fig, ax = plt.subplots(1,3 if test else 2, figsize=(13,5))\n",
    "    \n",
    "    # plot classifcation regions\n",
    "    grid=1000\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "    X_ = np.array([np.ravel(xx), np.ravel(yy)]).T\n",
    "    if not norm is None:\n",
    "        zz = model.decision_function(norm.transform(X_)).reshape(grid,grid)\n",
    "    else:\n",
    "        zz = model.decision_function(X_).reshape(xx.shape)\n",
    "\n",
    "    for i in range(3 if test else 2):\n",
    "        ax[i].imshow(\n",
    "            zz,\n",
    "            interpolation=\"nearest\",\n",
    "            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "            aspect=\"auto\",\n",
    "            alpha=0.5,\n",
    "            origin=\"lower\",\n",
    "            cmap='rocket_r',\n",
    "        )\n",
    "        # cs = plt.contourf(xx,yy,zz, zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "        # plt.contour(cs, colors='k')\n",
    "\n",
    "    # plot data points\n",
    "    sns.scatterplot(ax=ax[0], x=X_train[:,0], y=X_train[:,1], hue=y_train,\n",
    "                    style=y_train, markers={'M': 'X', 'B': 'o'}, \n",
    "                    palette='rocket', hue_order=['M', 'B'])\n",
    "    ax[0].set_title('Training Data')\n",
    "    ax[0].set_xlabel(features[0])\n",
    "    ax[0].set_ylabel(features[1])\n",
    "\n",
    "    sns.scatterplot(ax=ax[1], x=X_val[:,0], y=X_val[:,1], hue=y_val,\n",
    "                    style=y_val, markers={'M': 'X', 'B': 'o'}, \n",
    "                    palette='rocket', hue_order=['M', 'B'])\n",
    "    ax[1].set_title('Validation Data')\n",
    "    ax[1].set_xlabel(features[0])\n",
    "    ax[1].set_ylabel(features[1])\n",
    "\n",
    "    if test:\n",
    "        sns.scatterplot(ax=ax[2], x=X_test[:,0], y=X_test[:,1], hue=y_test,\n",
    "                        style=y_test, markers={'M': 'X', 'B': 'o'}, \n",
    "                        palette='rocket', hue_order=['M', 'B'])\n",
    "        ax[2].set_title('Test Data')\n",
    "        ax[2].set_xlabel(features[0])\n",
    "        ax[2].set_ylabel(features[1])\n",
    "    # s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M'][x1], cancer_df[cancer_df[\"diagnosis\"]=='M'][x2], marker='x', color='C3')\n",
    "    # s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B'][x1], cancer_df[cancer_df[\"diagnosis\"]=='B'][x2], marker='+', color='C0')    \n",
    "    # plt.xlim(x_range)\n",
    "    # plt.ylim(y_range)\n",
    "    # plt.xlabel(x1)\n",
    "    # plt.ylabel(x2)\n",
    "    # plt.legend([s1,s2],['Malignant','Benign'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the defined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "features = ('smoothness_mean','compactness_se', 'id') # we keep the ID around for plotting the different sets\n",
    "\n",
    "# Read in the data set and choose features to use\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.reset_index(inplace=True)\n",
    "X = np.array(cancer_df[[*features]])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# split the data set for cross-validation\n",
    "X_train, X_, y_train, y_ = train_test_split(X, Y, train_size = 0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size = 0.5)\n",
    "\n",
    "# fit a model with RBF features (kernel)\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "# do a first run without scaling and hyperparameter tuning\n",
    "aio_helper(*features[:2], model, norm=None, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we see that the two features we chose have one problem: They make for no good decision boundaries. So let's try some other features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "features = ('perimeter_worst','concave points_worst', 'id') # we keep the ID around for plotting the different sets\n",
    "\n",
    "# Read in the data set and choose features to use\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.reset_index(inplace=True)\n",
    "X = np.array(cancer_df[[*features]])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# split the data set for cross-validation\n",
    "X_train, X_, y_train, y_ = train_test_split(X, Y, train_size = 0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size = 0.5)\n",
    "\n",
    "# fit a model with RBF features (kernel)\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "# do a first run without scaling and hyperparameter tuning\n",
    "aio_helper(*features[:2], model, norm=None, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this got us a linear decision boundary - even though we were using RBFs... How is that possible?\n",
    "The answer lies in the scales of the dimensions: The decision boundary is basically only dictated by changes in x1 (`perimeter_worst`).\n",
    "Let's introduce scaling to alleviate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "features = ('perimeter_worst','concave points_worst', 'id') # we keep the ID around for plotting the different sets\n",
    "\n",
    "# Read in the data set and choose features to use\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.reset_index(inplace=True)\n",
    "X = np.array(cancer_df[[*features]])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# split the data set for cross-validation\n",
    "X_train, X_, y_train, y_ = train_test_split(X, Y, train_size = 0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size = 0.5)\n",
    "\n",
    "# fit a model with RBF features (kernel)\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "# do a first run without scaling and hyperparameter tuning\n",
    "aio_helper(*features[:2], model, norm=StandardScaler(), test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we managed to get the scaling right and the scores a little higher, this looks overly regularized..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "features = ('perimeter_worst','concave points_worst', 'id') # we keep the ID around for plotting the different sets\n",
    "\n",
    "# Read in the data set and choose features to use\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.reset_index(inplace=True)\n",
    "X = np.array(cancer_df[[*features]])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# split the data set for cross-validation\n",
    "X_train, X_, y_train, y_ = train_test_split(X, Y, train_size = 0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size = 0.5)\n",
    "\n",
    "# fit a model with RBF features (kernel) and less strict regularization\n",
    "# hint: with sklearn SVM the C parameter is inversely proportional to regularization strength!\n",
    "model = SVC(kernel='rbf', C=10)\n",
    "\n",
    "# do a first run without scaling and hyperparameter tuning\n",
    "aio_helper(*features[:2], model, norm=StandardScaler(), test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we like this, so let's repeat this, but this time let's look at the test score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "features = ('perimeter_worst','concave points_worst', 'id') # we keep the ID around for plotting the different sets\n",
    "\n",
    "# Read in the data set and choose features to use\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.reset_index(inplace=True)\n",
    "X = np.array(cancer_df[[*features]])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# split the data set for cross-validation\n",
    "X_train, X_, y_train, y_ = train_test_split(X, Y, train_size = 0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size = 0.5)\n",
    "\n",
    "# fit a model with RBF features (kernel) and less strict regularization\n",
    "# hint: with sklearn SVM the C parameter is inversely proportional to regularization strength!\n",
    "model = SVC(kernel='rbf', C=10)\n",
    "\n",
    "# do a first run without scaling and hyperparameter tuning\n",
    "aio_helper(*features[:2], model, norm=StandardScaler(), test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test score looks good! Doesn't indicate overfitting to either the training data or the hyperparameters!\n",
    "(Note: In reality, we would've wanted to re-train the model using training and validation data mixed to compute the test scores!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How was I able to come up with two features that can achieve 95% accuracy? See below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X_new = SelectKBest(f_classif, k=2).fit_transform(np.array(cancer_df.drop(columns=['diagnosis'])), Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222.6666717529297px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
